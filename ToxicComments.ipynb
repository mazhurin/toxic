{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of Toxic Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle competition: [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mazhurin/keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,GlobalMaxPool1D,Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_comments(filename = ''):\n",
    "    comments = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        header = True\n",
    "        for row in csvReader:\n",
    "            if header:\n",
    "                header = False\n",
    "                continue\n",
    "            ids.append(row[0])\n",
    "            s = row[1].translate(string.punctuation)\n",
    "            comments.append(s)          \n",
    "            if len(row) > 2:\n",
    "                classes = []\n",
    "                for i in range(2,8):\n",
    "                    classes.append(row[i])\n",
    "                labels.append(classes)\n",
    "\n",
    "    ids = np.asarray(ids)\n",
    "    X = np.asarray(comments)\n",
    "    Y = np.asarray(labels, dtype=int)\n",
    "\n",
    "    return ids, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train, X_train, Y_train = read_comments('./data/train.csv')\n",
    "id_test, X_test, _ = read_comments('./data/test.csv')\n",
    "labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "\n",
    "max_comment = 0\n",
    "for i in range(X_train.shape[0]):                               \n",
    "    l = len(X_train[i].lower().split())\n",
    "    if max_comment < l:\n",
    "        max_comment = l\n",
    "\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5,random_state=seed)\n",
    "# folds = list()\n",
    "# for train_index, dev_index in kfold.split(X_train):\n",
    "#     folds.append({\n",
    "#         'train_index' : train_index,\n",
    "#         'dev_index' : dev_index,     \n",
    "#     })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec = read_embeddings('./glove/glove.twitter.27B.25d.txt')\n",
    "word_to_vec[\"0.065581\"] = [ 0.39605,  -0.96669,   0.23706,  -0.41379,  -0.97006,   0.16601,  -1.292,\n",
    " -0.58989,   0.11632,  -1.365,    -0.27939,  -0.57222,  -0.97108,  -0.56319,\n",
    " -0.015263, -0.70465,  -0.13867 ,  1.0702 ,  -0.25557  , 0.25122,  -0.87755,\n",
    "  0.70999 ,  0.9118 ,  -0.30077, 0 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fistfuckee.\n",
      "fuck.\n",
      "fuck.++You\n",
      "motherfucker!!!\n",
      "++fuck\n",
      "fuck.\n",
      "Assfuckers\n",
      "muthafucker.\n",
      "\"\"fucking\n",
      "++Muthafucka\n",
      "fuck.\n",
      "dumbfucks.\n",
      "muthafucka!\n",
      "mothafucka!\n",
      "clusterfucked\n",
      "\"\"clusterfucked\n",
      "motherfucker.\n",
      "\"\"fucking\n",
      "fuck...Before\n",
      "http://gawker.com/5636765/facebook-ceo-admits-to-calling-users-dumb-fucks+\n",
      "http://www.theweek.co.uk/technology/14625/are-users-%E2%80%98dumb-fucks%E2%80%99-trusting-data-facebook+\n",
      "http://tdh.me/zuckerberg-called-early-facebook-users-dumb-fucks-so-what/+\n",
      "http://anphicle.com/en/they-trust-me-dumb-fucks-facebook-ceo-mark-zuckerberg/+\n",
      "fucks\"\".\n",
      "fucks.++In\n",
      "Corpsefucking\n",
      "Corpsefucking\n",
      "Corpsefucking\n",
      "Corpsefucking\n",
      "changes++fuck\n",
      "motherfucker\"\"\n",
      "dumass.++fuck\n",
      "\"\"fucking\n",
      "fuckkk\n",
      "\"\"mindfuck\"\"\n",
      "fuck's\n",
      "Mindfuck\n",
      "fuck?\n",
      "fuckface.\n",
      "fuck.\n",
      "fuckin'\n",
      "motherfucker,\n",
      "bitch.++fuck\n",
      "lord/fuck\n",
      "fuckersItalic\n",
      "dumbfuck,\n",
      "fuck,\n",
      "fuck????\n",
      "fucking-ass\n",
      "++fuck\n",
      "Merchan-fucking-dise\n",
      "T-fucking-800.\n",
      "fuck...damn\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck,\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fucking\"\")\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "fuck.\n",
      "'fuck'\n",
      "\"\"fuck.\"\"\n",
      ".++fuck\n",
      "fucktard.\n",
      "http://denverfuckedme.com/FY/viewtopic.php?f=7&t;=180&p;=365#p365++NOTICE!\n",
      "motherfucker?\n",
      "motherfucker}}\n",
      "++fuck\n",
      "Bg(fucking)\n",
      "\"\"fuck\n",
      "shitfucker+-me++Take\n",
      "dickfucker\n",
      "motherfucker!\n",
      "motherfucker,\n",
      "motherfucker.\n",
      "fuck?\"\n",
      "\"\"fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "fuck......will\n",
      "animalfucker\n",
      "animalfucker.\n",
      "fuck?!?!++\"\"Germany\"\"\n",
      "fucker!!\n",
      "fuck.++You\n",
      "edits.++fuck\n",
      "fuck.++I\n",
      "fuck,\n",
      "dumbfuck.\n",
      "fucks.\n",
      "fuckhead!\n",
      "fucktard,\n",
      "motherfucker.\n",
      "fuck.+Should\n",
      "mother-fucker\n",
      "fuck.\n",
      "motherfucker,\n",
      "++fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+computers\n",
      "\"\"fuck,\n",
      "fuckily\n",
      "\"\"fuck\n",
      "fuck,\n",
      "\"\"whoop-de-fucking-do\"\"?\n",
      "fucka!!!\n",
      "motherfuckers.++Typical\n",
      "fuckAN\n",
      "fucker,\n",
      "society...fuck\n",
      "fuck-sissy!!!!!!!!!++\n",
      "fuckin'\n",
      "fuck...\n",
      "Yamla_likes_to_fuck_babies_up_the_ass_wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\n",
      "++fucks\n",
      "++fuck\n",
      "fuck!\n",
      "clusterfuck.\n",
      "fuck!\n",
      "fuck!\n",
      "Corspefucking\n",
      "'fuck'\n",
      "++fuck\n",
      "http://gawker.com/5636765/facebook-ceo-admits-to-calling-users-dumb-fucks+\n",
      "http://www.theweek.co.uk/technology/14625/are-users-%E2%80%98dumb-fucks%E2%80%99-trusting-data-facebook+\n",
      "http://tdh.me/zuckerberg-called-early-facebook-users-dumb-fucks-so-what/+\n",
      "http://anphicle.com/en/they-trust-me-dumb-fucks-facebook-ceo-mark-zuckerberg/+\n",
      "fucks\"\".\n",
      "fucks.++In\n",
      "fuck?\n",
      "fuck+Got\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "dumb-fucks,\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "fuck?\n",
      "motherfucker:++1.\n",
      "fuck!!!!!!\"\n",
      "fuck?\n",
      "fuck.+And\n",
      "fuck,\n",
      "fucker+Shut\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fucker.\n",
      "dumbfuck?\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "fucki9ng\n",
      "++fuck\n",
      "fucker!126.12.157.168\n",
      "fuckstain.\n",
      "fuckhead!\n",
      "fuckturd!\n",
      "fucks,\n",
      "Motherfucking\n",
      "sick....fuckkkkk\n",
      "\"\"fuck\n",
      "fuckwit;++http://www.telegraph.co.uk/sport/othersports/boxing/11626770/James-DeGale-claims-super-middleweight-title-with-points-win-over-Andre-Dirrell-in-Boston.html++http://www.independent.co.uk/sport/general/boxing/andre-dirrell-vs-james-degale-on-saturday-degale-can-join-britains-great-list-of-supermiddleweight-champions-says-steve-bunce-10259295.html++FYI....boxing\n",
      "fuck!!\n",
      "'fuck'\n",
      "fuck,\n",
      "fuckyourself!!!\n",
      "motherfucker!!!!\n",
      "motherfucker...only\n",
      "fuck!ng\n",
      "fuck!ng\n",
      "fuck!ng\n",
      "fuck!ng\n",
      "'fuck'.\n",
      "'fuck'\n",
      "'fuck'\n",
      "fuckwad\"\"\n",
      "fuck.\n",
      "fuckknuckle?\n",
      "fuckknuckle?\n",
      "motherfucker!\n",
      "motherfuck,\n",
      "\"\"fucking\"\"\n",
      "\"\"fucking\"\"\n",
      "dumbfuck.\n",
      "fuck,\n",
      "motherfucker!!!\n",
      "motherfuckers?\n",
      "\"\"fuck\n",
      "fuckass.\n",
      "fuck,\n",
      "fuck.\n",
      "fuck.\n",
      "fuckin,ill\n",
      "\"\"fuck\n",
      "fuckin'\n",
      "fuck.\n",
      "fucked?\n",
      "Motherfucker\n",
      "fuck.\n",
      "bullshit+fuck\n",
      "fuckin'\n",
      "fuck's\n",
      "'fuck\n",
      "fuckwad.\n",
      "motherfucker\"\"\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin't\n",
      "\"\"fuck\n",
      "motherfucker?\n",
      "fuck\"\n",
      "much...fucked\n",
      "fuckmor\n",
      "++fuckBold\n",
      "fuck.\n",
      "fuckyouself\n",
      "fuckin'\n",
      "fuck.\n",
      "fat-fuck\n",
      "++fuck\n",
      "death++fuck\n",
      "\"\"fuckperezhilton.com\"\"\n",
      "\"\"fuck\n",
      "fucker,\n",
      "fuck.\n",
      "motherfucker.\n",
      "fucker.\n",
      "fucks,\n",
      "++fuck\n",
      "motherfucker.\n",
      "fuckwit!\n",
      "bitchfuck.\n",
      "fuckface.\n",
      "fuckfest\"\"\n",
      "fuck.\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fucker,\n",
      "me++Motherfucker++Ugh\n",
      "motherfucker.\n",
      "mohterfuck\n",
      "fuckwad.\"\n",
      "fuck!++Fuck\n",
      "fuckhead!\n",
      "fucker.\n",
      "fuck'ing\n",
      "fucker,\n",
      "whateverthefuck,++You\n",
      "fuck!\n",
      "Motherfucker\n",
      "Motherfucker,\n",
      "fuck,\n",
      "fucker,\n",
      "fuck,\n",
      "fist-fuckee\n",
      "dumbfuck.\n",
      "\"\"fuck\"\"\n",
      "fuckin'\n",
      "++fuck\n",
      "fuck?\n",
      "pigfuckers++There's\n",
      "fucktarded\n",
      "\"\"you-are-fucking-crazy-get-out-before-we-send-you-a-whole-jarful-of-cookies\n",
      "fucker.\n",
      "motherfucker.\n",
      "http://gawker.com/5636765/facebook-ceo-admits-to-calling-users-dumb-fucks+\n",
      "http://www.theweek.co.uk/technology/14625/are-users-%E2%80%98dumb-fucks%E2%80%99-trusting-data-facebook+\n",
      "http://tdh.me/zuckerberg-called-early-facebook-users-dumb-fucks-so-what/+\n",
      "http://anphicle.com/en/they-trust-me-dumb-fucks-facebook-ceo-mark-zuckerberg/+\n",
      "fucks\"\".\n",
      "fucks.++In\n",
      "fuck?\n",
      "mutherfuckng\n",
      "fuck,\n",
      "++fuck\n",
      "\"\"fucker\"\"\n",
      "motherfucker.\n",
      "\"\"fuckhead\"\"\n",
      "fuck.\n",
      "fuck.\n",
      "lord/fuck\n",
      "fucka!\n",
      "fuckwit!++Hu,you\n",
      "motherfucker!\n",
      "mother'fuckerer!!\n",
      "Deadfuck.\n",
      "\"\"fuck\n",
      "++fuck\n",
      "wikipedians++fuck\n",
      "motherfucker:++1.\n",
      "fuck!!!!!!\"\n",
      "fuckin'\n",
      "bitchfuck.\n",
      "fuckface.\n",
      "fuckyourself!!!++all\n",
      "\"+fuck\n",
      "fuckin'\n",
      "fucktard!!!!!!!\n",
      "fuckin'\n",
      "fuck.\n",
      "fuck.\n",
      "Motherfucker\n",
      "fuck?\n",
      "\"fucking\n",
      "fucknuckle\n",
      "fucknuckle\n",
      "motherfucker,\n",
      "fuck.\n",
      "fuck...damn\n",
      "fuckin'\n",
      "fuckin'\n",
      "MOtherfucker,\n",
      "fuckers,\n",
      "fuck,\n",
      "Motherfucker\n",
      "fuck-all\n",
      "motherfucker,\n",
      "shitfucker+-me\n",
      "fucked-ass\n",
      "fuck?!\n",
      "fuckwit!!!\n",
      "fuck,\n",
      "++fuck\n",
      "\"\"fuck\n",
      "Motherfucker,\n",
      "\"\"fuck\"\"\"\"\n",
      "ass-fuckers?\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "fuck,\n",
      "fuck.\n",
      "ISP...++yawn...++fuckin\n",
      "fuck.\n",
      "UTfuckin\n",
      "fucknig\n",
      "fuckyourself!!!\n",
      "fuck!\n",
      "motherfucker.\n",
      "++fuck\n",
      "\"\"fuck\n",
      "fucker...all\n",
      "motherfuckers.\n",
      "fuck!\n",
      "++youfuckingidiot\n",
      "motherfucker.\n",
      "sir...fucking\n",
      "fuck.\n",
      "fuck.\n",
      "fuck.\n",
      "discofucker,\n",
      "mothafucka.\n",
      "fuckin'\n",
      "++fuck??\n",
      "cow-fucking\n",
      "'fucker'\n",
      "fuck.\n",
      "fucker!\n",
      "'fucking',\n",
      "'fuck',\n",
      "asshole+fuck\n",
      "motherfucker,son\n",
      "fucked.\n",
      "fucker.\n",
      "fucker.\n",
      "fuck's\n",
      "brain++fuck\n",
      "motherfucker.\n",
      "motherfucker.\n",
      "\"\"fuck\n",
      "motherfucker:++1.\n",
      "fuck!!!!!!\"\n",
      "++pecker++fuckstick!!!++\n",
      "fuck-knuckle.\n",
      "fucker!!\n",
      "fuckin'\n",
      "fuckwit!\n",
      "fuckin'\n",
      "fuck.\n",
      "motherfucker,\n",
      "ass-fucked\n",
      ",otherfucker\n",
      "fuck:\n",
      "fuck?\n",
      "fuck,\n",
      "fuck.\n",
      "fucker.\n",
      "fucker?\n",
      "fuckface,\n",
      "fuckz.+[[Unblock}}\n",
      "motherfucker.\n",
      "\"\"fucking\n",
      "fuckwad.\n",
      "fuckers.\n",
      "fuck.\n",
      "motherfucker.\n",
      "fuckwit.\n",
      "motherfucker.\n",
      "fuckin'\n",
      "fuck++love\n",
      "fuck.\n",
      "xxxxxxMotherfucker+I\n",
      "fucker.\n",
      "fuckwads.\n",
      "fuckwads\n",
      "animalfucker\n",
      "muthafucker!\n",
      "fuck!120.62.16.147\n",
      "fuck?!\n",
      "\"\"fucking\n",
      "motherfucker.there\n",
      "++fuck\n",
      "fuck's\n",
      "LICKER+fuck\n",
      "fuck\"\"\n",
      "fucked,\n",
      "\"\"fuck\"\"\n",
      "you++fuck\n",
      "PEE-WEES++fuckan\n",
      "fucker.\n",
      "fuck,\n",
      "fuckhead.\n",
      "fuckelwad?\n",
      "fuck,\n",
      "fuck-it-I-don't-give-a-fuck\n",
      "fuck.\n",
      "fuck.\"\n",
      "motherfucker,\n",
      "motherfuckers++They\n",
      "fuck.+~~\n",
      "\"\"fucking\n",
      "fuckass!+You\n",
      "Yingfuck\n",
      "fuckin'\n",
      "fucknut.\n",
      "\":::fuck\n",
      "'fuck\n",
      "fuckyourselves+blaming\n",
      "everyfucking\n",
      "fucker,\n",
      "motherfucker!\n",
      "Mindfuck,\n",
      "clusterfuck,\n",
      "sockfuck.\n",
      "fuck.\n",
      "fucker.\n",
      "fucker.\n",
      "motherfucker++You\n",
      "fucker|\n",
      "fuckin'\n",
      "titt-fucking\n",
      "fucks,\n",
      "fuckwit.\n",
      "*fucking*\n",
      "fucks,\n",
      "fuck-head.\n",
      "fuckwit.\n",
      "fucker..Rajputs\n",
      "++fuck\n",
      "cousinfuckers\n",
      "motherfuckers.\n",
      "fuck?\n",
      "fuck's\n",
      "fuckers,\n",
      "Motherfucker\n",
      "motherfucker,\n",
      "++fucker\n",
      "fuck,\n",
      "motherfucker!\n",
      "Priest++motherfucker++shit+shit\n",
      "Motherfucker\n",
      "fuck'in\n",
      "fuck'in\n",
      "fuckiest\n",
      "fuckin'\n",
      "++fuck\n",
      "\"++fuck\n",
      "Motherfucker\n",
      "fucker.\n",
      "fucking-ass\n",
      "fuckwit!!!+Don;t\n",
      "mean?++fucking\n",
      "Pictures!!!++Motherfuckers\n",
      "fall++gfuck++good\n",
      "motehrfucker.\n",
      "fuck.\n",
      "fucktard.\n",
      "motherfucker,\n",
      "motherfucker,\n",
      "fuckin'\n",
      "fuck?\n",
      "fuck?\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fuck\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "motherfucker,\n",
      "Assfuckers\n",
      "++fuck\n",
      "fuck?\n",
      "fuck.\n",
      "motherfucker...\n",
      "++fuck\n",
      "Bamafuck\n",
      "is\"\"F-f-f-fuck\n",
      "fuckwit?\n",
      "un-fuck\n",
      ",fuck\n",
      "fuck.\n",
      "fucking.\n",
      "motherfuckin'\n",
      "fuckin'\n",
      "fucked.\n",
      "++fuck\n",
      "fuck's\n",
      "\"\"fuck\n",
      "fuckıng\n",
      "fuckıng\n",
      "fuck,\n",
      "fuckin'\n",
      "dog-fucking.++\"\n",
      "fucker.!!!!\n",
      "\"\"fuck\n",
      "fuckbags\n",
      "fuckwads.\n",
      "fuckwads\n",
      "\"\"fuck\n",
      "fuck,\n",
      "\"++fucking\n",
      "fuck!\n",
      "defuck+Your\n",
      "fuckability.\n",
      "mother-fucker\n",
      "fucker.\n",
      "Swagfuckingtastic.++\n",
      "fuck.\n",
      "fuckiest\n",
      "fuckin'\n",
      "fuckiest\n",
      "fuckin'\n",
      "fuckhead.\n",
      "asshole++fuck\n",
      "fist-fucking.\n",
      "motherfucker.\n",
      "+fuck\n",
      "(fucking\n",
      "fuckchop\n",
      "you+fuck\n",
      "motherfucker.\n",
      "fuck,\n",
      "fuck,\n",
      "fuck.\n",
      "fuck,\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "motherfucker!!!\n",
      "fuck.\n",
      "motherfucker,\n",
      "fuck,\n",
      "\"\"fuck\"\"\n",
      "fucking...\n",
      "fuck!!!!!\n",
      "\"\"fuck\"\"\n",
      "fucker.\n",
      "fuck.\n",
      "fuckwit.\n",
      "fuckwqit\n",
      "++fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "fuck.\n",
      "24-fucking-7\n",
      "motherfucker!!!!!!!!\"\"++Something\n",
      "fuck43\n",
      "fuck.\n",
      "fucktard.\n",
      "\"++Motherfucka\n",
      "motherfucker!!!\n",
      "fuckers.\n",
      "fucker's\n",
      "fuckmehorns,\n",
      "++fuck\n",
      "\"\"fuckism\"\"?\n",
      "fucktard.\n",
      "\"\"fuck\"\"\n",
      "fuck,\n",
      "shut-the-fuck-up\n",
      "fuck's\n",
      "fucker!!!!\n",
      "fucker.\n",
      "fuck'n\n",
      "fuck.\n",
      "fuckin'\n",
      "fuckin'\n",
      "++fuck\n",
      "fuck.\n",
      "fucker.++\"\n",
      "fuck,\n",
      "fuckin'\n",
      "site......fuck\n",
      "Huh?!?++fuck\n",
      "fuckin'\n",
      "me+Motherfucker+Ugh\"\n",
      "fuck.\n",
      "LifeBaka+fuck\n",
      "DIE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!fucker\n",
      "nonononononjesusfuckingchristno!\n",
      "\"\"fuck\"\"?\n",
      "fuckin'\n",
      "fucked.\n",
      "fuckers,\n",
      "fucker.\n",
      "fuck.\n",
      "++fuck\n",
      "++fuck\n",
      "fuckers!\n",
      "fucks.\n",
      "++fuck\n",
      "Al-fucking-mighty\n",
      "censored...++fuck\n",
      "fuck?\n",
      "++fuck\n",
      "++fuck\n",
      "++fuck\n",
      "animalfucker.\n",
      "fucktards??\n",
      "fuckface,\n",
      "fucker.\n",
      "++fuck\n",
      "\"\"fuck\"\"\n",
      "motherfuckers.\n",
      "fuck!\n",
      "fucktards??\n",
      "fuckfu*ker.\n",
      "fuckhead,+Contributions\n",
      "fuck.\n",
      "fuckwhits!!!\n",
      "fuck\"\";\n",
      "fucker!!!\n",
      "fucks,\n",
      "User:gofuckyourself\n",
      "fuck,\n",
      "\"\"fucking\"\"\n",
      "fuck-head,\n",
      "Pinkafuckup\n",
      "fucker.\n",
      "fucktard.\n",
      "fucker,\n",
      "fucktard.\n",
      "\"\"fuckin\"\"\n",
      "fuckup.\n",
      "fucktards.\n",
      "motherfucker,\n",
      "motherfucker.\n",
      "fucktard.\n",
      "me.++fuck\n",
      "child.fuck\n",
      "fuck.)\n",
      "fuckwhit\n",
      "fuckface?89.123.100.99\n",
      "\"\"fuck\"\"\n",
      "fuck!?+Computer:\n",
      "\"\"fuck\"\"\n",
      "shit-fucking\n",
      "fucks.\n",
      "Motherfucker\n",
      "Motherfucker,\n",
      "fuck,\n",
      "fucker,\n",
      "fuck,\n",
      "motherfuckers!\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck-sya\"\"\n",
      "fucktard.\n",
      "\"\"fuck\"\",\n",
      "\"\"fucked\n",
      "fuckin'\n",
      "fuckin'\n",
      "fucked.\n",
      "moth*rfucker,\n",
      "++fuck\n",
      "motherfucker.\n",
      "\"\"fuck\"\"\n",
      "fuck.\n",
      "Motherfucking\n",
      "fucks,\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "motherfucker!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mum!++fuck\n",
      "too++fuck\n",
      "++fuck\n",
      "++fuck\n",
      "don't-give-a-fuck\n",
      "(fucking\n",
      "motherfucker?\n",
      "motherfucker!!!!!!!!!!!!!\n",
      "motherfucker.\n",
      "fuck.\n",
      "motherfucker.\n",
      "fuck+Got\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuck+Got\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuck+Got\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuck+Got\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fucked.\n",
      "\"\"fucking\n",
      "fuckwit.\n",
      "Motherfucking\n",
      "Motherfucker.+Example:\n",
      "Motherfucking\n",
      "fuckstick.\n",
      "fuck.\n",
      "motherfucker.\n",
      "fuck,\n",
      "\"\"fuck\n",
      "\"\"fucking\"\"\n",
      "\"\"fucking\"\"\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "fuck.\n",
      "lord/fuck\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "fucker!!!\n",
      "what-so-fucking-ever!!!\n",
      "dickhead....fuck\n",
      "dickhead....fuck\n",
      "dickhead....fuck\n",
      "dickhead....fuck\n",
      "dickhead....fuck\n",
      "dickhead....fuck\n",
      "fuckish\n",
      "fucker!!\n",
      "fuck?\n",
      "(fuck\n",
      "Motherfucking\n",
      "fucked.\n",
      "fuckin'\n",
      "dumbfuck+The\n",
      "fuck's\n",
      "fuck!!!\n",
      "{{unblock-auto|...}fuck\n",
      "\"\"fuck\n",
      "fuck.\n",
      "website.++fuck\n",
      "\"\"clusterfucked\n",
      "motherfucker.\n",
      "fuck-sucking\n",
      "julianfuckton?\n",
      "dog-fucking?\n",
      "examples!+http://propagandapress.org/2007/02/21/fuck-bubba-by-saab-lofton/+http://propagandapress.org/2007/02/26/man-fuck-florida-talking-shit-about-cuba-saab-lofton/+http://activistsinlasvegas.blogspot.com/2006/12/rules-for-whites-by-saab-lofton.html\n",
      "motherfucker.\n",
      "Dumbfuck\n",
      "motherfucker.\n",
      "fuck.\n",
      "fuck-head.\n",
      "fuck.\n",
      "talk:Larryfuckstylinson\n",
      "muthafuckaa+How\n",
      "drugged/fucked\n",
      "fuckin'\n",
      "fuck?\n",
      "fucker.\n",
      "fuck,\n",
      "fucka..what\n",
      "bitch.....++fuck\n",
      "buhay..,fuck\n",
      "fuckwit.\n",
      "fucker.\n",
      "fucker!!!\n",
      "http://gawker.com/5636765/facebook-ceo-admits-to-calling-users-dumb-fucks+\n",
      "http://www.theweek.co.uk/technology/14625/are-users-%E2%80%98dumb-fucks%E2%80%99-trusting-data-facebook+\n",
      "http://tdh.me/zuckerberg-called-early-facebook-users-dumb-fucks-so-what/+\n",
      "http://anphicle.com/en/they-trust-me-dumb-fucks-facebook-ceo-mark-zuckerberg/+\n",
      "fucks\"\".\n",
      "fucks.++In\n",
      "fucka.+So\n",
      "fucka....\n",
      "fucking!\n",
      "fuck.\n",
      "fucks,\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "++fuckoff\n",
      "Motherfucked++you\n",
      "me++Motherfucker++Ugh\"\n",
      "motherfucker,\n",
      "fuck.\n",
      "fucker!\n",
      "pigfucks,\n",
      "fuck.\n",
      "motherfucker.\n",
      "\\fuck\n",
      "fucked.\n",
      "fucker.\"\n",
      "\"\"fuck\"\"\n",
      "fuckers!!!!\n",
      "fuckin'\n",
      "motherfucker,\n",
      "fuck!\n",
      "fucker.\n",
      "fuck's\n",
      "\"\"clusterfucked\n",
      "\"\"clusterfuck\"\"\n",
      "Ratfucking\n",
      "\"\"fuck\"\"?\n",
      "\"\"fuck\n",
      "\"\"fucking\"\"\n",
      "fuck!\n",
      "++fuck\n",
      "fucks,\n",
      "bullshit+fuck\n",
      "motherfucker.\n",
      "\"\"fucking\"\"\n",
      "\"\"fucking\"\"\n",
      "mo'fucka!!\"\n",
      "'fuck'\n",
      "'fuck'\n",
      "\"\"fuck\"\"\n",
      "fuck(that\n",
      "fuckwitz\n",
      "fuckitz\n",
      "fucker????\n",
      "fuck:++It\n",
      "++Motherfucker\n",
      "motherfucker.\n",
      "\"\"fuck\n",
      "motherfucker.\n",
      "fuck,\n",
      "Congrad-a-fucking-lations\n",
      "fuck,\n",
      "dumbfuck.\n",
      "http://gawker.com/5636765/facebook-ceo-admits-to-calling-users-dumb-fucks+\n",
      "http://www.theweek.co.uk/technology/14625/are-users-%E2%80%98dumb-fucks%E2%80%99-trusting-data-facebook+\n",
      "http://tdh.me/zuckerberg-called-early-facebook-users-dumb-fucks-so-what/+\n",
      "http://anphicle.com/en/they-trust-me-dumb-fucks-facebook-ceo-mark-zuckerberg/+\n",
      "fucks\"\".\n",
      "fucks.++In\n",
      "fuck,\n",
      "fucks,\n",
      "++fuck\n",
      "motherfucker,\n",
      "motherfuck.\n",
      "textfuck\n",
      "motherfucker,even\n",
      "++fuck\n",
      "motherfucker.\n",
      "fuck,\n",
      "'fucking'.\n",
      "fuck.\n",
      "(fucking)\n",
      "weeds.\"\"+fuck\n"
     ]
    }
   ],
   "source": [
    "for r in X_train:\n",
    "    words = r.split()\n",
    "    for w in words:\n",
    "        if w not in word_to_index:\n",
    "            if 'fuck' in w:\n",
    "                print(w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is naive model based on averaging all word embeddings in a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_comment(comment, word_to_vec):\n",
    "    \"\"\"\n",
    "    Converts a comment into a list of words. \n",
    "    Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    comment -- string\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary \n",
    "    into its n-dimensional vector representation\n",
    "    \n",
    "    Returns:\n",
    "    avg -- average vector encoding information about the sentence, numpy-array of shape (n,)\n",
    "    \"\"\"\n",
    "    words = comment[0].lower().split()\n",
    "    avg = np.zeros(shape=(len(word_to_vec['apple']), ))\n",
    "    for w in words:\n",
    "        if w in word_to_vec:\n",
    "            avg += word_to_vec[w]\n",
    "    avg = avg / len(words)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = list()\n",
    "for c in X_train:\n",
    "    averages.append(average_comment(c, word_to_vec))\n",
    "averages = np.asarray(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mazhurin/keras/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "averages_test = list()\n",
    "for c in X_test:\n",
    "    averages_test.append(average_comment(c, word_to_vec))\n",
    "averages_test = np.asarray(averages_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from keras.backend import int_shape\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "_EPSILON = K.epsilon()\n",
    "    \n",
    "def custom_loss(y_true, y_pred):\n",
    "    losses = []\n",
    "    y_pred = K.clip(y_pred, _EPSILON, 1.0-_EPSILON)\n",
    "    num_classes = int_shape(y_pred)[1]\n",
    "    for i in range(num_classes):\n",
    "        losses.append(K.mean(-(\n",
    "            y_true[:, i]*K.log(y_pred[:, i]) - (1-y_true[:, i])*K.log(1-y_pred[:, i])\n",
    "        ), axis = -1))\n",
    "    loss = tf.stack(losses)\n",
    "    loss = K.mean(loss, axis=-1)\n",
    "        \n",
    "    return loss\n",
    "    #return tf.convert_to_tensor(losses, dtype='float32')\n",
    "\n",
    "def baseline_model(num_dim=25, num_labels=6):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_labels, input_shape=(num_dim,), kernel_initializer='normal', \n",
    "                    activation='sigmoid'))\n",
    "    #model.add(Activation('sigmoid'))\n",
    "    model.compile(\n",
    "        #loss='binary_crossentropy', \n",
    "        loss=custom_loss,\n",
    "        optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 35008/159571 [=====>........................] - ETA: 12s - loss: -7.2864 - acc: 0.0599"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-a36c5d050f16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2475\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(averages, Y_train, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(averages[3003:3005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[3003:3005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "estimator_baseline = KerasRegressor(build_fn=baseline_model, \n",
    "                                    nb_epoch=1, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 42035/127656 [========>.....................] - ETA: 36s - loss: -14.1116 - acc: 0.8558"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-5b653b2bace7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results: %.2f (%.2f) MSE\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = cross_val_score(estimator_baseline, averages, Y_train, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999994 , 0.99999964, 0.9999995 , 0.99999964, 0.9999995 ,\n",
       "        0.99999964]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.asarray([average_comment(\"Love You\", word_to_vec)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    X_indices = np.zeros(shape=(m, max_len))\n",
    "    for i in range(m):                               \n",
    "        sentence_words = X[i].lower().split()\n",
    "        if len(sentence_words) > max_len:\n",
    "            print (\"Too many words:\", len(sentence_words))\n",
    "            continue\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            else :\n",
    "                X_indices[i, j] = word_to_index['word']\n",
    "            j = j + 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "2321\n",
      "1667\n",
      "1667\n"
     ]
    }
   ],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, max_comment)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe \n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_len = len(word_to_index) + 1                  \n",
    "    emb_dim = word_to_vec[\"cucumber\"].shape[0]  \n",
    "    emb_matrix = np.zeros(shape=(vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(input_shape, num_classes, word_to_vec, word_to_index):\n",
    "    \"\"\"\n",
    "    Create the model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec -- dictionary mapping every word in a vocabulary into its n-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary \n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    sentence_indices = Input(shape=input_shape, dtype='float32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    X = Bidirectional(LSTM(18, return_sequences = False))(embeddings)\n",
    "    #X = GlobalMaxPool1D()(X)\n",
    "    X = Dropout(rate=0.1)(X)\n",
    "#     X = LSTM(8, return_sequences = False)(X)\n",
    "#     X = Dropout(rate=0.2)(X)\n",
    "    X = Dense(units=50, activation='relu')(X)\n",
    "    X = Dropout(0.1)(X)\n",
    "    X = Dense(units=num_classes, activation='sigmoid')(X)\n",
    "    model = Model(inputs = [sentence_indices], outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1411)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 1411, 25)          29837875  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 36)                6336      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                1850      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 29,846,367\n",
      "Trainable params: 8,492\n",
      "Non-trainable params: 29,837,875\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_RNN = rnn_model((max_comment,), num_classes, word_to_vec, word_to_index)\n",
    "model_RNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  3456/159571 [..............................] - ETA: 32:08 - loss: 0.1069 - acc: 0.9650"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b9e9bb176d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_RNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_RNN.fit(X_train_indices, Y_train, epochs = 3, batch_size = 128, shuffle=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[3003:3005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2932739 , 0.03831418, 0.16043979, 0.01002212, 0.13886277,\n",
       "        0.02675362],\n",
       "       [0.5962145 , 0.09602976, 0.5305401 , 0.0326421 , 0.47383296,\n",
       "        0.10741065]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_RNN.predict(X_train_indices[3003:3005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53518003, 0.0736991 , 0.40560737, 0.02496896, 0.4049595 ,\n",
       "        0.073121  ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_RNN.predict(\n",
    "    sentences_to_indices(\n",
    "        np.asarray([\n",
    "            \"I will kill you, motherfucker\"]), \n",
    "        word_to_index, max_comment\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model_RNN.predict(X_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([\n",
    "    pd.DataFrame(id_test, columns=['id']), \n",
    "    pd.DataFrame(y, columns=labels)], axis=1)\n",
    "result = result.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001cee341fdb12</th>\n",
       "      <td>0.999972</td>\n",
       "      <td>0.999977</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000247867823ef7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00013b17ad220c46</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017563c3f7919a</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017695ad8997eb</th>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001ea8717f6de06</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00024115d4cbde0f</th>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000247e83dcc1211</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00025358d4737918</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00026d1092fe71cc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002eadc3b301559</th>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002f87b16116a7f</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003806b11932181</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003e1cccfd5a40a</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00059ace3e3e9a53</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000634272d0d44eb</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000663aff0fffc80</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000689dd34e20979</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000834769115370c</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000844b52dee5f3f</th>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00084da5d4ead7aa</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00091c35fa9d0465</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000968ce11f5ee34</th>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0009734200a85047</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00097b6214686db5</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0009aef4bd9e1697</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000a02d807ae0254</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000a6c6d4e89b9bc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000bafe2080bba82</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000bf0a9894b2807</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff3ae2e177b6bb3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff4109e837f7acc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff4373a81ef9f2a</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff460574ddbcd80</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff4fc0a1555be5c</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff5b9bb944d634c</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff5c4a77fe0c05f</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff5fb61bd637c82</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff69311f306df44</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff6ad63666fb304</th>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff7159b3ee95618</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff718ffe5f05559</th>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff7fc22a0cdccd3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff83b80284d8440</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff8ef316d0c6990</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff8f521a7dbcd47</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff8f64043129fa2</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff9d70fe0722906</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff9fa508f400ee6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffa3fae1890b40a</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffa8a11c4378854</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffac2a094c8e0e2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffb5451268fb5ba</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffc2b34bbe61c8d</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffc489742ffe69b</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffcd0960ee309b5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffd7a9a6eb32c16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffda9e8d6fafa9e</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffe8f1340a79fc2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffce3fb183ee80</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "id                                                                       \n",
       "00001cee341fdb12  0.999972      0.999977  0.999979  0.999974  0.999985   \n",
       "0000247867823ef7  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "00013b17ad220c46  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "00017563c3f7919a  0.999999      0.999999  0.999999  0.999999  0.999999   \n",
       "00017695ad8997eb  0.999990      0.999990  0.999988  0.999991  0.999987   \n",
       "0001ea8717f6de06  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "00024115d4cbde0f  0.999998      0.999999  0.999999  0.999999  0.999999   \n",
       "000247e83dcc1211  0.999999      0.999999  0.999999  0.999999  0.999999   \n",
       "00025358d4737918  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "00026d1092fe71cc  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "0002eadc3b301559  0.999990      0.999990  0.999988  0.999991  0.999987   \n",
       "0002f87b16116a7f  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "0003806b11932181  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "0003e1cccfd5a40a  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "00059ace3e3e9a53  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "000634272d0d44eb  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "000663aff0fffc80  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "000689dd34e20979  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "000834769115370c  0.999999      0.999999  0.999999  0.999999  0.999999   \n",
       "000844b52dee5f3f  0.999994      0.999995  0.999993  0.999994  0.999994   \n",
       "00084da5d4ead7aa  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "00091c35fa9d0465  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "000968ce11f5ee34  0.999998      0.999999  0.999999  0.999999  0.999999   \n",
       "0009734200a85047  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "00097b6214686db5  0.999999      0.999999  0.999999  0.999999  0.999999   \n",
       "0009aef4bd9e1697  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "000a02d807ae0254  0.999995      0.999995  0.999994  0.999995  0.999994   \n",
       "000a6c6d4e89b9bc  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "000bafe2080bba82  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "000bf0a9894b2807  0.999999      0.999999  0.999999  0.999999  0.999999   \n",
       "...                    ...           ...       ...       ...       ...   \n",
       "fff3ae2e177b6bb3  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fff4109e837f7acc  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fff4373a81ef9f2a  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fff460574ddbcd80  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fff4fc0a1555be5c  0.999999      0.999999  0.999999  0.999999  0.999999   \n",
       "fff5b9bb944d634c  0.999999      0.999999  0.999999  0.999999  0.999999   \n",
       "fff5c4a77fe0c05f  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fff5fb61bd637c82  0.999999      0.999999  0.999999  0.999999  0.999999   \n",
       "fff69311f306df44  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fff6ad63666fb304  0.999997      0.999997  0.999997  0.999998  0.999998   \n",
       "fff7159b3ee95618  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fff718ffe5f05559  0.999990      0.999990  0.999988  0.999991  0.999987   \n",
       "fff7fc22a0cdccd3  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fff83b80284d8440  0.999999      0.999999  0.999999  0.999999  0.999999   \n",
       "fff8ef316d0c6990  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fff8f521a7dbcd47  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fff8f64043129fa2  0.999999      0.999999  0.999999  0.999999  0.999999   \n",
       "fff9d70fe0722906  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fff9fa508f400ee6  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fffa3fae1890b40a  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fffa8a11c4378854  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fffac2a094c8e0e2  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fffb5451268fb5ba  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fffc2b34bbe61c8d  0.999999      0.999999  0.999999  0.999999  0.999999   \n",
       "fffc489742ffe69b  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fffcd0960ee309b5  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fffd7a9a6eb32c16  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fffda9e8d6fafa9e  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "fffe8f1340a79fc2  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "ffffce3fb183ee80  1.000000      1.000000  1.000000  1.000000  1.000000   \n",
       "\n",
       "                  identity_hate  \n",
       "id                               \n",
       "00001cee341fdb12       0.999987  \n",
       "0000247867823ef7       1.000000  \n",
       "00013b17ad220c46       1.000000  \n",
       "00017563c3f7919a       0.999999  \n",
       "00017695ad8997eb       0.999990  \n",
       "0001ea8717f6de06       1.000000  \n",
       "00024115d4cbde0f       0.999999  \n",
       "000247e83dcc1211       0.999999  \n",
       "00025358d4737918       1.000000  \n",
       "00026d1092fe71cc       1.000000  \n",
       "0002eadc3b301559       0.999990  \n",
       "0002f87b16116a7f       1.000000  \n",
       "0003806b11932181       1.000000  \n",
       "0003e1cccfd5a40a       1.000000  \n",
       "00059ace3e3e9a53       1.000000  \n",
       "000634272d0d44eb       1.000000  \n",
       "000663aff0fffc80       1.000000  \n",
       "000689dd34e20979       1.000000  \n",
       "000834769115370c       0.999999  \n",
       "000844b52dee5f3f       0.999990  \n",
       "00084da5d4ead7aa       1.000000  \n",
       "00091c35fa9d0465       1.000000  \n",
       "000968ce11f5ee34       0.999999  \n",
       "0009734200a85047       1.000000  \n",
       "00097b6214686db5       0.999999  \n",
       "0009aef4bd9e1697       1.000000  \n",
       "000a02d807ae0254       0.999994  \n",
       "000a6c6d4e89b9bc       1.000000  \n",
       "000bafe2080bba82       1.000000  \n",
       "000bf0a9894b2807       0.999999  \n",
       "...                         ...  \n",
       "fff3ae2e177b6bb3       1.000000  \n",
       "fff4109e837f7acc       1.000000  \n",
       "fff4373a81ef9f2a       1.000000  \n",
       "fff460574ddbcd80       1.000000  \n",
       "fff4fc0a1555be5c       0.999999  \n",
       "fff5b9bb944d634c       0.999999  \n",
       "fff5c4a77fe0c05f       1.000000  \n",
       "fff5fb61bd637c82       0.999999  \n",
       "fff69311f306df44       1.000000  \n",
       "fff6ad63666fb304       0.999998  \n",
       "fff7159b3ee95618       1.000000  \n",
       "fff718ffe5f05559       0.999990  \n",
       "fff7fc22a0cdccd3       1.000000  \n",
       "fff83b80284d8440       0.999999  \n",
       "fff8ef316d0c6990       1.000000  \n",
       "fff8f521a7dbcd47       1.000000  \n",
       "fff8f64043129fa2       0.999999  \n",
       "fff9d70fe0722906       1.000000  \n",
       "fff9fa508f400ee6       1.000000  \n",
       "fffa3fae1890b40a       1.000000  \n",
       "fffa8a11c4378854       1.000000  \n",
       "fffac2a094c8e0e2       1.000000  \n",
       "fffb5451268fb5ba       1.000000  \n",
       "fffc2b34bbe61c8d       0.999999  \n",
       "fffc489742ffe69b       1.000000  \n",
       "fffcd0960ee309b5       1.000000  \n",
       "fffd7a9a6eb32c16       1.000000  \n",
       "fffda9e8d6fafa9e       1.000000  \n",
       "fffe8f1340a79fc2       1.000000  \n",
       "ffffce3fb183ee80       1.000000  \n",
       "\n",
       "[153164 rows x 6 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./result/anton_base_{}.csv'.format(datetime.now()), header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>0.999977</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0002eadc3b301559</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0003806b11932181</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>000634272d0d44eb</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>000663aff0fffc80</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>000689dd34e20979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>000834769115370c</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>000844b52dee5f3f</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>00084da5d4ead7aa</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>00091c35fa9d0465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>000968ce11f5ee34</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0009734200a85047</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>00097b6214686db5</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0009aef4bd9e1697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>000a02d807ae0254</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>000a6c6d4e89b9bc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>000bafe2080bba82</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>000bf0a9894b2807</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153134</th>\n",
       "      <td>153134</td>\n",
       "      <td>fff3ae2e177b6bb3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153135</th>\n",
       "      <td>153135</td>\n",
       "      <td>fff4109e837f7acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153136</th>\n",
       "      <td>153136</td>\n",
       "      <td>fff4373a81ef9f2a</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153137</th>\n",
       "      <td>153137</td>\n",
       "      <td>fff460574ddbcd80</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153138</th>\n",
       "      <td>153138</td>\n",
       "      <td>fff4fc0a1555be5c</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153139</th>\n",
       "      <td>153139</td>\n",
       "      <td>fff5b9bb944d634c</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153140</th>\n",
       "      <td>153140</td>\n",
       "      <td>fff5c4a77fe0c05f</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153141</th>\n",
       "      <td>153141</td>\n",
       "      <td>fff5fb61bd637c82</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153142</th>\n",
       "      <td>153142</td>\n",
       "      <td>fff69311f306df44</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153143</th>\n",
       "      <td>153143</td>\n",
       "      <td>fff6ad63666fb304</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153144</th>\n",
       "      <td>153144</td>\n",
       "      <td>fff7159b3ee95618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153145</th>\n",
       "      <td>153145</td>\n",
       "      <td>fff718ffe5f05559</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153146</th>\n",
       "      <td>153146</td>\n",
       "      <td>fff7fc22a0cdccd3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153147</th>\n",
       "      <td>153147</td>\n",
       "      <td>fff83b80284d8440</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153148</th>\n",
       "      <td>153148</td>\n",
       "      <td>fff8ef316d0c6990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153149</th>\n",
       "      <td>153149</td>\n",
       "      <td>fff8f521a7dbcd47</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153150</th>\n",
       "      <td>153150</td>\n",
       "      <td>fff8f64043129fa2</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153151</th>\n",
       "      <td>153151</td>\n",
       "      <td>fff9d70fe0722906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153152</th>\n",
       "      <td>153152</td>\n",
       "      <td>fff9fa508f400ee6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153153</th>\n",
       "      <td>153153</td>\n",
       "      <td>fffa3fae1890b40a</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153154</th>\n",
       "      <td>153154</td>\n",
       "      <td>fffa8a11c4378854</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153155</th>\n",
       "      <td>153155</td>\n",
       "      <td>fffac2a094c8e0e2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153156</th>\n",
       "      <td>153156</td>\n",
       "      <td>fffb5451268fb5ba</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153157</th>\n",
       "      <td>153157</td>\n",
       "      <td>fffc2b34bbe61c8d</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153158</th>\n",
       "      <td>153158</td>\n",
       "      <td>fffc489742ffe69b</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>153159</td>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>153160</td>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>153161</td>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>153162</td>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>153163</td>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                id     toxic  severe_toxic   obscene    threat  \\\n",
       "0            0  00001cee341fdb12  0.999972      0.999977  0.999979  0.999974   \n",
       "1            1  0000247867823ef7  1.000000      1.000000  1.000000  1.000000   \n",
       "2            2  00013b17ad220c46  1.000000      1.000000  1.000000  1.000000   \n",
       "3            3  00017563c3f7919a  0.999999      0.999999  0.999999  0.999999   \n",
       "4            4  00017695ad8997eb  0.999990      0.999990  0.999988  0.999991   \n",
       "5            5  0001ea8717f6de06  1.000000      1.000000  1.000000  1.000000   \n",
       "6            6  00024115d4cbde0f  0.999998      0.999999  0.999999  0.999999   \n",
       "7            7  000247e83dcc1211  0.999999      0.999999  0.999999  0.999999   \n",
       "8            8  00025358d4737918  1.000000      1.000000  1.000000  1.000000   \n",
       "9            9  00026d1092fe71cc  1.000000      1.000000  1.000000  1.000000   \n",
       "10          10  0002eadc3b301559  0.999990      0.999990  0.999988  0.999991   \n",
       "11          11  0002f87b16116a7f  1.000000      1.000000  1.000000  1.000000   \n",
       "12          12  0003806b11932181  1.000000      1.000000  1.000000  1.000000   \n",
       "13          13  0003e1cccfd5a40a  1.000000      1.000000  1.000000  1.000000   \n",
       "14          14  00059ace3e3e9a53  1.000000      1.000000  1.000000  1.000000   \n",
       "15          15  000634272d0d44eb  1.000000      1.000000  1.000000  1.000000   \n",
       "16          16  000663aff0fffc80  1.000000      1.000000  1.000000  1.000000   \n",
       "17          17  000689dd34e20979  1.000000      1.000000  1.000000  1.000000   \n",
       "18          18  000834769115370c  0.999999      0.999999  0.999999  0.999999   \n",
       "19          19  000844b52dee5f3f  0.999994      0.999995  0.999993  0.999994   \n",
       "20          20  00084da5d4ead7aa  1.000000      1.000000  1.000000  1.000000   \n",
       "21          21  00091c35fa9d0465  1.000000      1.000000  1.000000  1.000000   \n",
       "22          22  000968ce11f5ee34  0.999998      0.999999  0.999999  0.999999   \n",
       "23          23  0009734200a85047  1.000000      1.000000  1.000000  1.000000   \n",
       "24          24  00097b6214686db5  0.999999      0.999999  0.999999  0.999999   \n",
       "25          25  0009aef4bd9e1697  1.000000      1.000000  1.000000  1.000000   \n",
       "26          26  000a02d807ae0254  0.999995      0.999995  0.999994  0.999995   \n",
       "27          27  000a6c6d4e89b9bc  1.000000      1.000000  1.000000  1.000000   \n",
       "28          28  000bafe2080bba82  1.000000      1.000000  1.000000  1.000000   \n",
       "29          29  000bf0a9894b2807  0.999999      0.999999  0.999999  0.999999   \n",
       "...        ...               ...       ...           ...       ...       ...   \n",
       "153134  153134  fff3ae2e177b6bb3  1.000000      1.000000  1.000000  1.000000   \n",
       "153135  153135  fff4109e837f7acc  1.000000      1.000000  1.000000  1.000000   \n",
       "153136  153136  fff4373a81ef9f2a  1.000000      1.000000  1.000000  1.000000   \n",
       "153137  153137  fff460574ddbcd80  1.000000      1.000000  1.000000  1.000000   \n",
       "153138  153138  fff4fc0a1555be5c  0.999999      0.999999  0.999999  0.999999   \n",
       "153139  153139  fff5b9bb944d634c  0.999999      0.999999  0.999999  0.999999   \n",
       "153140  153140  fff5c4a77fe0c05f  1.000000      1.000000  1.000000  1.000000   \n",
       "153141  153141  fff5fb61bd637c82  0.999999      0.999999  0.999999  0.999999   \n",
       "153142  153142  fff69311f306df44  1.000000      1.000000  1.000000  1.000000   \n",
       "153143  153143  fff6ad63666fb304  0.999997      0.999997  0.999997  0.999998   \n",
       "153144  153144  fff7159b3ee95618  1.000000      1.000000  1.000000  1.000000   \n",
       "153145  153145  fff718ffe5f05559  0.999990      0.999990  0.999988  0.999991   \n",
       "153146  153146  fff7fc22a0cdccd3  1.000000      1.000000  1.000000  1.000000   \n",
       "153147  153147  fff83b80284d8440  0.999999      0.999999  0.999999  0.999999   \n",
       "153148  153148  fff8ef316d0c6990  1.000000      1.000000  1.000000  1.000000   \n",
       "153149  153149  fff8f521a7dbcd47  1.000000      1.000000  1.000000  1.000000   \n",
       "153150  153150  fff8f64043129fa2  0.999999      0.999999  0.999999  0.999999   \n",
       "153151  153151  fff9d70fe0722906  1.000000      1.000000  1.000000  1.000000   \n",
       "153152  153152  fff9fa508f400ee6  1.000000      1.000000  1.000000  1.000000   \n",
       "153153  153153  fffa3fae1890b40a  1.000000      1.000000  1.000000  1.000000   \n",
       "153154  153154  fffa8a11c4378854  1.000000      1.000000  1.000000  1.000000   \n",
       "153155  153155  fffac2a094c8e0e2  1.000000      1.000000  1.000000  1.000000   \n",
       "153156  153156  fffb5451268fb5ba  1.000000      1.000000  1.000000  1.000000   \n",
       "153157  153157  fffc2b34bbe61c8d  0.999999      0.999999  0.999999  0.999999   \n",
       "153158  153158  fffc489742ffe69b  1.000000      1.000000  1.000000  1.000000   \n",
       "153159  153159  fffcd0960ee309b5  1.000000      1.000000  1.000000  1.000000   \n",
       "153160  153160  fffd7a9a6eb32c16  1.000000      1.000000  1.000000  1.000000   \n",
       "153161  153161  fffda9e8d6fafa9e  1.000000      1.000000  1.000000  1.000000   \n",
       "153162  153162  fffe8f1340a79fc2  1.000000      1.000000  1.000000  1.000000   \n",
       "153163  153163  ffffce3fb183ee80  1.000000      1.000000  1.000000  1.000000   \n",
       "\n",
       "          insult  identity_hate  \n",
       "0       0.999985       0.999987  \n",
       "1       1.000000       1.000000  \n",
       "2       1.000000       1.000000  \n",
       "3       0.999999       0.999999  \n",
       "4       0.999987       0.999990  \n",
       "5       1.000000       1.000000  \n",
       "6       0.999999       0.999999  \n",
       "7       0.999999       0.999999  \n",
       "8       1.000000       1.000000  \n",
       "9       1.000000       1.000000  \n",
       "10      0.999987       0.999990  \n",
       "11      1.000000       1.000000  \n",
       "12      1.000000       1.000000  \n",
       "13      1.000000       1.000000  \n",
       "14      1.000000       1.000000  \n",
       "15      1.000000       1.000000  \n",
       "16      1.000000       1.000000  \n",
       "17      1.000000       1.000000  \n",
       "18      0.999999       0.999999  \n",
       "19      0.999994       0.999990  \n",
       "20      1.000000       1.000000  \n",
       "21      1.000000       1.000000  \n",
       "22      0.999999       0.999999  \n",
       "23      1.000000       1.000000  \n",
       "24      0.999999       0.999999  \n",
       "25      1.000000       1.000000  \n",
       "26      0.999994       0.999994  \n",
       "27      1.000000       1.000000  \n",
       "28      1.000000       1.000000  \n",
       "29      0.999999       0.999999  \n",
       "...          ...            ...  \n",
       "153134  1.000000       1.000000  \n",
       "153135  1.000000       1.000000  \n",
       "153136  1.000000       1.000000  \n",
       "153137  1.000000       1.000000  \n",
       "153138  0.999999       0.999999  \n",
       "153139  0.999999       0.999999  \n",
       "153140  1.000000       1.000000  \n",
       "153141  0.999999       0.999999  \n",
       "153142  1.000000       1.000000  \n",
       "153143  0.999998       0.999998  \n",
       "153144  1.000000       1.000000  \n",
       "153145  0.999987       0.999990  \n",
       "153146  1.000000       1.000000  \n",
       "153147  0.999999       0.999999  \n",
       "153148  1.000000       1.000000  \n",
       "153149  1.000000       1.000000  \n",
       "153150  0.999999       0.999999  \n",
       "153151  1.000000       1.000000  \n",
       "153152  1.000000       1.000000  \n",
       "153153  1.000000       1.000000  \n",
       "153154  1.000000       1.000000  \n",
       "153155  1.000000       1.000000  \n",
       "153156  1.000000       1.000000  \n",
       "153157  0.999999       0.999999  \n",
       "153158  1.000000       1.000000  \n",
       "153159  1.000000       1.000000  \n",
       "153160  1.000000       1.000000  \n",
       "153161  1.000000       1.000000  \n",
       "153162  1.000000       1.000000  \n",
       "153163  1.000000       1.000000  \n",
       "\n",
       "[153164 rows x 8 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(averages_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
