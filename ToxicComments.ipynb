{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of Toxic Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle competition: [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mazhurin/keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,GlobalMaxPool1D,Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_comments(filename = ''):\n",
    "    comments = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        header = True\n",
    "        for row in csvReader:\n",
    "            if header:\n",
    "                header = False\n",
    "                continue\n",
    "            ids.append(row[0])\n",
    "            s = row[1].translate(string.punctuation)\n",
    "            comments.append(s)          \n",
    "            if len(row) > 2:\n",
    "                classes = []\n",
    "                for i in range(2,8):\n",
    "                    classes.append(row[i])\n",
    "                labels.append(classes)\n",
    "\n",
    "    ids = np.asarray(ids)\n",
    "    X = np.asarray(comments)\n",
    "    Y = np.asarray(labels, dtype=int)\n",
    "\n",
    "    return ids, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train, X_train, Y_train = read_comments('./data/train.csv')\n",
    "id_test, X_test, _ = read_comments('./data/test.csv')\n",
    "labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "\n",
    "max_comment = 0\n",
    "for i in range(X_train.shape[0]):                               \n",
    "    l = len(X_train[i].split())\n",
    "    if max_comment < l:\n",
    "        max_comment = l\n",
    "for i in range(X_test.shape[0]):                               \n",
    "    l = len(X_test[i].split())\n",
    "    if max_comment < l:\n",
    "        max_comment = l\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5,random_state=seed)\n",
    "# folds = list()\n",
    "# for train_index, dev_index in kfold.split(X_train):\n",
    "#     folds.append({\n",
    "#         'train_index' : train_index,\n",
    "#         'dev_index' : dev_index,     \n",
    "#     })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec = read_embeddings('./glove/glove.twitter.27B.25d.txt')\n",
    "word_to_vec[\"0.065581\"] = [ 0.39605,  -0.96669,   0.23706,  -0.41379,  -0.97006,   0.16601,  -1.292,\n",
    " -0.58989,   0.11632,  -1.365,    -0.27939,  -0.57222,  -0.97108,  -0.56319,\n",
    " -0.015263, -0.70465,  -0.13867 ,  1.0702 ,  -0.25557  , 0.25122,  -0.87755,\n",
    "  0.70999 ,  0.9118 ,  -0.30077, 0 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fistfuckee.\n",
      "fuck.\n",
      "fuck.++You\n",
      "motherfucker!!!\n",
      "++fuck\n",
      "fuck.\n",
      "Assfuckers\n",
      "muthafucker.\n",
      "\"\"fucking\n",
      "++Muthafucka\n",
      "fuck.\n",
      "dumbfucks.\n",
      "muthafucka!\n",
      "mothafucka!\n",
      "clusterfucked\n",
      "\"\"clusterfucked\n",
      "motherfucker.\n",
      "\"\"fucking\n",
      "fuck...Before\n",
      "http://gawker.com/5636765/facebook-ceo-admits-to-calling-users-dumb-fucks+\n",
      "http://www.theweek.co.uk/technology/14625/are-users-%E2%80%98dumb-fucks%E2%80%99-trusting-data-facebook+\n",
      "http://tdh.me/zuckerberg-called-early-facebook-users-dumb-fucks-so-what/+\n",
      "http://anphicle.com/en/they-trust-me-dumb-fucks-facebook-ceo-mark-zuckerberg/+\n",
      "fucks\"\".\n",
      "fucks.++In\n",
      "Corpsefucking\n",
      "Corpsefucking\n",
      "Corpsefucking\n",
      "Corpsefucking\n",
      "changes++fuck\n",
      "motherfucker\"\"\n",
      "dumass.++fuck\n",
      "\"\"fucking\n",
      "fuckkk\n",
      "\"\"mindfuck\"\"\n",
      "fuck's\n",
      "Mindfuck\n",
      "fuck?\n",
      "fuckface.\n",
      "fuck.\n",
      "fuckin'\n",
      "motherfucker,\n",
      "bitch.++fuck\n",
      "lord/fuck\n",
      "fuckersItalic\n",
      "dumbfuck,\n",
      "fuck,\n",
      "fuck????\n",
      "fucking-ass\n",
      "++fuck\n",
      "Merchan-fucking-dise\n",
      "T-fucking-800.\n",
      "fuck...damn\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck,\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fucking\"\")\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "fuck.\n",
      "'fuck'\n",
      "\"\"fuck.\"\"\n",
      ".++fuck\n",
      "fucktard.\n",
      "http://denverfuckedme.com/FY/viewtopic.php?f=7&t;=180&p;=365#p365++NOTICE!\n",
      "motherfucker?\n",
      "motherfucker}}\n",
      "++fuck\n",
      "Bg(fucking)\n",
      "\"\"fuck\n",
      "shitfucker+-me++Take\n",
      "dickfucker\n",
      "motherfucker!\n",
      "motherfucker,\n",
      "motherfucker.\n",
      "fuck?\"\n",
      "\"\"fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "bitches.fuck\n",
      "fuck......will\n",
      "animalfucker\n",
      "animalfucker.\n",
      "fuck?!?!++\"\"Germany\"\"\n",
      "fucker!!\n",
      "fuck.++You\n",
      "edits.++fuck\n",
      "fuck.++I\n",
      "fuck,\n",
      "dumbfuck.\n",
      "fucks.\n",
      "fuckhead!\n",
      "fucktard,\n",
      "motherfucker.\n",
      "fuck.+Should\n",
      "mother-fucker\n",
      "fuck.\n",
      "motherfucker,\n",
      "++fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck+computers\n",
      "\"\"fuck,\n",
      "fuckily\n",
      "\"\"fuck\n",
      "fuck,\n",
      "\"\"whoop-de-fucking-do\"\"?\n",
      "fucka!!!\n",
      "motherfuckers.++Typical\n",
      "fuckAN\n",
      "fucker,\n",
      "society...fuck\n",
      "fuck-sissy!!!!!!!!!++\n",
      "fuckin'\n",
      "fuck...\n",
      "Yamla_likes_to_fuck_babies_up_the_ass_wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\n",
      "++fucks\n",
      "++fuck\n",
      "fuck!\n",
      "clusterfuck.\n",
      "fuck!\n",
      "fuck!\n",
      "Corspefucking\n",
      "'fuck'\n",
      "++fuck\n",
      "http://gawker.com/5636765/facebook-ceo-admits-to-calling-users-dumb-fucks+\n",
      "http://www.theweek.co.uk/technology/14625/are-users-%E2%80%98dumb-fucks%E2%80%99-trusting-data-facebook+\n",
      "http://tdh.me/zuckerberg-called-early-facebook-users-dumb-fucks-so-what/+\n",
      "http://anphicle.com/en/they-trust-me-dumb-fucks-facebook-ceo-mark-zuckerberg/+\n",
      "fucks\"\".\n",
      "fucks.++In\n",
      "fuck?\n",
      "fuck+Got\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "dumb-fucks,\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "fuck?\n",
      "motherfucker:++1.\n",
      "fuck!!!!!!\"\n",
      "fuck?\n",
      "fuck.+And\n",
      "fuck,\n",
      "fucker+Shut\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fucker.\n",
      "dumbfuck?\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "fucki9ng\n",
      "++fuck\n",
      "fucker!126.12.157.168\n",
      "fuckstain.\n",
      "fuckhead!\n",
      "fuckturd!\n",
      "fucks,\n",
      "Motherfucking\n",
      "sick....fuckkkkk\n",
      "\"\"fuck\n",
      "fuckwit;++http://www.telegraph.co.uk/sport/othersports/boxing/11626770/James-DeGale-claims-super-middleweight-title-with-points-win-over-Andre-Dirrell-in-Boston.html++http://www.independent.co.uk/sport/general/boxing/andre-dirrell-vs-james-degale-on-saturday-degale-can-join-britains-great-list-of-supermiddleweight-champions-says-steve-bunce-10259295.html++FYI....boxing\n",
      "fuck!!\n",
      "'fuck'\n",
      "fuck,\n",
      "fuckyourself!!!\n",
      "motherfucker!!!!\n",
      "motherfucker...only\n",
      "fuck!ng\n",
      "fuck!ng\n",
      "fuck!ng\n",
      "fuck!ng\n",
      "'fuck'.\n",
      "'fuck'\n",
      "'fuck'\n",
      "fuckwad\"\"\n",
      "fuck.\n",
      "fuckknuckle?\n",
      "fuckknuckle?\n",
      "motherfucker!\n",
      "motherfuck,\n",
      "\"\"fucking\"\"\n",
      "\"\"fucking\"\"\n",
      "dumbfuck.\n",
      "fuck,\n",
      "motherfucker!!!\n",
      "motherfuckers?\n",
      "\"\"fuck\n",
      "fuckass.\n",
      "fuck,\n",
      "fuck.\n",
      "fuck.\n",
      "fuckin,ill\n",
      "\"\"fuck\n",
      "fuckin'\n",
      "fuck.\n",
      "fucked?\n",
      "Motherfucker\n",
      "fuck.\n",
      "bullshit+fuck\n",
      "fuckin'\n",
      "fuck's\n",
      "'fuck\n",
      "fuckwad.\n",
      "motherfucker\"\"\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin't\n",
      "\"\"fuck\n",
      "motherfucker?\n",
      "fuck\"\n",
      "much...fucked\n",
      "fuckmor\n",
      "++fuckBold\n",
      "fuck.\n",
      "fuckyouself\n",
      "fuckin'\n",
      "fuck.\n",
      "fat-fuck\n",
      "++fuck\n",
      "death++fuck\n",
      "\"\"fuckperezhilton.com\"\"\n",
      "\"\"fuck\n",
      "fucker,\n",
      "fuck.\n",
      "motherfucker.\n",
      "fucker.\n",
      "fucks,\n",
      "++fuck\n",
      "motherfucker.\n",
      "fuckwit!\n",
      "bitchfuck.\n",
      "fuckface.\n",
      "fuckfest\"\"\n",
      "fuck.\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fucker,\n",
      "me++Motherfucker++Ugh\n",
      "motherfucker.\n",
      "mohterfuck\n",
      "fuckwad.\"\n",
      "fuck!++Fuck\n",
      "fuckhead!\n",
      "fucker.\n",
      "fuck'ing\n",
      "fucker,\n",
      "whateverthefuck,++You\n",
      "fuck!\n",
      "Motherfucker\n",
      "Motherfucker,\n",
      "fuck,\n",
      "fucker,\n",
      "fuck,\n",
      "fist-fuckee\n",
      "dumbfuck.\n",
      "\"\"fuck\"\"\n",
      "fuckin'\n",
      "++fuck\n",
      "fuck?\n",
      "pigfuckers++There's\n",
      "fucktarded\n",
      "\"\"you-are-fucking-crazy-get-out-before-we-send-you-a-whole-jarful-of-cookies\n",
      "fucker.\n",
      "motherfucker.\n",
      "http://gawker.com/5636765/facebook-ceo-admits-to-calling-users-dumb-fucks+\n",
      "http://www.theweek.co.uk/technology/14625/are-users-%E2%80%98dumb-fucks%E2%80%99-trusting-data-facebook+\n",
      "http://tdh.me/zuckerberg-called-early-facebook-users-dumb-fucks-so-what/+\n",
      "http://anphicle.com/en/they-trust-me-dumb-fucks-facebook-ceo-mark-zuckerberg/+\n",
      "fucks\"\".\n",
      "fucks.++In\n",
      "fuck?\n",
      "mutherfuckng\n",
      "fuck,\n",
      "++fuck\n",
      "\"\"fucker\"\"\n",
      "motherfucker.\n",
      "\"\"fuckhead\"\"\n",
      "fuck.\n",
      "fuck.\n",
      "lord/fuck\n",
      "fucka!\n",
      "fuckwit!++Hu,you\n",
      "motherfucker!\n",
      "mother'fuckerer!!\n",
      "Deadfuck.\n",
      "\"\"fuck\n",
      "++fuck\n",
      "wikipedians++fuck\n",
      "motherfucker:++1.\n",
      "fuck!!!!!!\"\n",
      "fuckin'\n",
      "bitchfuck.\n",
      "fuckface.\n",
      "fuckyourself!!!++all\n",
      "\"+fuck\n",
      "fuckin'\n",
      "fucktard!!!!!!!\n",
      "fuckin'\n",
      "fuck.\n",
      "fuck.\n",
      "Motherfucker\n",
      "fuck?\n",
      "\"fucking\n",
      "fucknuckle\n",
      "fucknuckle\n",
      "motherfucker,\n",
      "fuck.\n",
      "fuck...damn\n",
      "fuckin'\n",
      "fuckin'\n",
      "MOtherfucker,\n",
      "fuckers,\n",
      "fuck,\n",
      "Motherfucker\n",
      "fuck-all\n",
      "motherfucker,\n",
      "shitfucker+-me\n",
      "fucked-ass\n",
      "fuck?!\n",
      "fuckwit!!!\n",
      "fuck,\n",
      "++fuck\n",
      "\"\"fuck\n",
      "Motherfucker,\n",
      "\"\"fuck\"\"\"\"\n",
      "ass-fuckers?\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "fuck,\n",
      "fuck.\n",
      "ISP...++yawn...++fuckin\n",
      "fuck.\n",
      "UTfuckin\n",
      "fucknig\n",
      "fuckyourself!!!\n",
      "fuck!\n",
      "motherfucker.\n",
      "++fuck\n",
      "\"\"fuck\n",
      "fucker...all\n",
      "motherfuckers.\n",
      "fuck!\n",
      "++youfuckingidiot\n",
      "motherfucker.\n",
      "sir...fucking\n",
      "fuck.\n",
      "fuck.\n",
      "fuck.\n",
      "discofucker,\n",
      "mothafucka.\n",
      "fuckin'\n",
      "++fuck??\n",
      "cow-fucking\n",
      "'fucker'\n",
      "fuck.\n",
      "fucker!\n",
      "'fucking',\n",
      "'fuck',\n",
      "asshole+fuck\n",
      "motherfucker,son\n",
      "fucked.\n",
      "fucker.\n",
      "fucker.\n",
      "fuck's\n",
      "brain++fuck\n",
      "motherfucker.\n",
      "motherfucker.\n",
      "\"\"fuck\n",
      "motherfucker:++1.\n",
      "fuck!!!!!!\"\n",
      "++pecker++fuckstick!!!++\n",
      "fuck-knuckle.\n",
      "fucker!!\n",
      "fuckin'\n",
      "fuckwit!\n",
      "fuckin'\n",
      "fuck.\n",
      "motherfucker,\n",
      "ass-fucked\n",
      ",otherfucker\n",
      "fuck:\n",
      "fuck?\n",
      "fuck,\n",
      "fuck.\n",
      "fucker.\n",
      "fucker?\n",
      "fuckface,\n",
      "fuckz.+[[Unblock}}\n",
      "motherfucker.\n",
      "\"\"fucking\n",
      "fuckwad.\n",
      "fuckers.\n",
      "fuck.\n",
      "motherfucker.\n",
      "fuckwit.\n",
      "motherfucker.\n",
      "fuckin'\n",
      "fuck++love\n",
      "fuck.\n",
      "xxxxxxMotherfucker+I\n",
      "fucker.\n",
      "fuckwads.\n",
      "fuckwads\n",
      "animalfucker\n",
      "muthafucker!\n",
      "fuck!120.62.16.147\n",
      "fuck?!\n",
      "\"\"fucking\n",
      "motherfucker.there\n",
      "++fuck\n",
      "fuck's\n",
      "LICKER+fuck\n",
      "fuck\"\"\n",
      "fucked,\n",
      "\"\"fuck\"\"\n",
      "you++fuck\n",
      "PEE-WEES++fuckan\n",
      "fucker.\n",
      "fuck,\n",
      "fuckhead.\n",
      "fuckelwad?\n",
      "fuck,\n",
      "fuck-it-I-don't-give-a-fuck\n",
      "fuck.\n",
      "fuck.\"\n",
      "motherfucker,\n",
      "motherfuckers++They\n",
      "fuck.+~~\n",
      "\"\"fucking\n",
      "fuckass!+You\n",
      "Yingfuck\n",
      "fuckin'\n",
      "fucknut.\n",
      "\":::fuck\n",
      "'fuck\n",
      "fuckyourselves+blaming\n",
      "everyfucking\n",
      "fucker,\n",
      "motherfucker!\n",
      "Mindfuck,\n",
      "clusterfuck,\n",
      "sockfuck.\n",
      "fuck.\n",
      "fucker.\n",
      "fucker.\n",
      "motherfucker++You\n",
      "fucker|\n",
      "fuckin'\n",
      "titt-fucking\n",
      "fucks,\n",
      "fuckwit.\n",
      "*fucking*\n",
      "fucks,\n",
      "fuck-head.\n",
      "fuckwit.\n",
      "fucker..Rajputs\n",
      "++fuck\n",
      "cousinfuckers\n",
      "motherfuckers.\n",
      "fuck?\n",
      "fuck's\n",
      "fuckers,\n",
      "Motherfucker\n",
      "motherfucker,\n",
      "++fucker\n",
      "fuck,\n",
      "motherfucker!\n",
      "Priest++motherfucker++shit+shit\n",
      "Motherfucker\n",
      "fuck'in\n",
      "fuck'in\n",
      "fuckiest\n",
      "fuckin'\n",
      "++fuck\n",
      "\"++fuck\n",
      "Motherfucker\n",
      "fucker.\n",
      "fucking-ass\n",
      "fuckwit!!!+Don;t\n",
      "mean?++fucking\n",
      "Pictures!!!++Motherfuckers\n",
      "fall++gfuck++good\n",
      "motehrfucker.\n",
      "fuck.\n",
      "fucktard.\n",
      "motherfucker,\n",
      "motherfucker,\n",
      "fuckin'\n",
      "fuck?\n",
      "fuck?\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fuck\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "fuck+Fucky\n",
      "motherfucker,\n",
      "Assfuckers\n",
      "++fuck\n",
      "fuck?\n",
      "fuck.\n",
      "motherfucker...\n",
      "++fuck\n",
      "Bamafuck\n",
      "is\"\"F-f-f-fuck\n",
      "fuckwit?\n",
      "un-fuck\n",
      ",fuck\n",
      "fuck.\n",
      "fucking.\n",
      "motherfuckin'\n",
      "fuckin'\n",
      "fucked.\n",
      "++fuck\n",
      "fuck's\n",
      "\"\"fuck\n",
      "fuckıng\n",
      "fuckıng\n",
      "fuck,\n",
      "fuckin'\n",
      "dog-fucking.++\"\n",
      "fucker.!!!!\n",
      "\"\"fuck\n",
      "fuckbags\n",
      "fuckwads.\n",
      "fuckwads\n",
      "\"\"fuck\n",
      "fuck,\n",
      "\"++fucking\n",
      "fuck!\n",
      "defuck+Your\n",
      "fuckability.\n",
      "mother-fucker\n",
      "fucker.\n",
      "Swagfuckingtastic.++\n",
      "fuck.\n",
      "fuckiest\n",
      "fuckin'\n",
      "fuckiest\n",
      "fuckin'\n",
      "fuckhead.\n",
      "asshole++fuck\n",
      "fist-fucking.\n",
      "motherfucker.\n",
      "+fuck\n",
      "(fucking\n",
      "fuckchop\n",
      "you+fuck\n",
      "motherfucker.\n",
      "fuck,\n",
      "fuck,\n",
      "fuck.\n",
      "fuck,\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "motherfucker!!!\n",
      "fuck.\n",
      "motherfucker,\n",
      "fuck,\n",
      "\"\"fuck\"\"\n",
      "fucking...\n",
      "fuck!!!!!\n",
      "\"\"fuck\"\"\n",
      "fucker.\n",
      "fuck.\n",
      "fuckwit.\n",
      "fuckwqit\n",
      "++fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "cunt+fuck\n",
      "fuck.\n",
      "24-fucking-7\n",
      "motherfucker!!!!!!!!\"\"++Something\n",
      "fuck43\n",
      "fuck.\n",
      "fucktard.\n",
      "\"++Motherfucka\n",
      "motherfucker!!!\n",
      "fuckers.\n",
      "fucker's\n",
      "fuckmehorns,\n",
      "++fuck\n",
      "\"\"fuckism\"\"?\n",
      "fucktard.\n",
      "\"\"fuck\"\"\n",
      "fuck,\n",
      "shut-the-fuck-up\n",
      "fuck's\n",
      "fucker!!!!\n",
      "fucker.\n",
      "fuck'n\n",
      "fuck.\n",
      "fuckin'\n",
      "fuckin'\n",
      "++fuck\n",
      "fuck.\n",
      "fucker.++\"\n",
      "fuck,\n",
      "fuckin'\n",
      "site......fuck\n",
      "Huh?!?++fuck\n",
      "fuckin'\n",
      "me+Motherfucker+Ugh\"\n",
      "fuck.\n",
      "LifeBaka+fuck\n",
      "DIE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!fucker\n",
      "nonononononjesusfuckingchristno!\n",
      "\"\"fuck\"\"?\n",
      "fuckin'\n",
      "fucked.\n",
      "fuckers,\n",
      "fucker.\n",
      "fuck.\n",
      "++fuck\n",
      "++fuck\n",
      "fuckers!\n",
      "fucks.\n",
      "++fuck\n",
      "Al-fucking-mighty\n",
      "censored...++fuck\n",
      "fuck?\n",
      "++fuck\n",
      "++fuck\n",
      "++fuck\n",
      "animalfucker.\n",
      "fucktards??\n",
      "fuckface,\n",
      "fucker.\n",
      "++fuck\n",
      "\"\"fuck\"\"\n",
      "motherfuckers.\n",
      "fuck!\n",
      "fucktards??\n",
      "fuckfu*ker.\n",
      "fuckhead,+Contributions\n",
      "fuck.\n",
      "fuckwhits!!!\n",
      "fuck\"\";\n",
      "fucker!!!\n",
      "fucks,\n",
      "User:gofuckyourself\n",
      "fuck,\n",
      "\"\"fucking\"\"\n",
      "fuck-head,\n",
      "Pinkafuckup\n",
      "fucker.\n",
      "fucktard.\n",
      "fucker,\n",
      "fucktard.\n",
      "\"\"fuckin\"\"\n",
      "fuckup.\n",
      "fucktards.\n",
      "motherfucker,\n",
      "motherfucker.\n",
      "fucktard.\n",
      "me.++fuck\n",
      "child.fuck\n",
      "fuck.)\n",
      "fuckwhit\n",
      "fuckface?89.123.100.99\n",
      "\"\"fuck\"\"\n",
      "fuck!?+Computer:\n",
      "\"\"fuck\"\"\n",
      "shit-fucking\n",
      "fucks.\n",
      "Motherfucker\n",
      "Motherfucker,\n",
      "fuck,\n",
      "fucker,\n",
      "fuck,\n",
      "motherfuckers!\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck\"\"\n",
      "\"\"fuck-sya\"\"\n",
      "fucktard.\n",
      "\"\"fuck\"\",\n",
      "\"\"fucked\n",
      "fuckin'\n",
      "fuckin'\n",
      "fucked.\n",
      "moth*rfucker,\n",
      "++fuck\n",
      "motherfucker.\n",
      "\"\"fuck\"\"\n",
      "fuck.\n",
      "Motherfucking\n",
      "fucks,\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "fucksex\n",
      "motherfucker!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mum!++fuck\n",
      "too++fuck\n",
      "++fuck\n",
      "++fuck\n",
      "don't-give-a-fuck\n",
      "(fucking\n",
      "motherfucker?\n",
      "motherfucker!!!!!!!!!!!!!\n",
      "motherfucker.\n",
      "fuck.\n",
      "motherfucker.\n",
      "fuck+Got\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuck+Got\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuck+Got\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuck+Got\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fuckin'\n",
      "fucked.\n",
      "\"\"fucking\n",
      "fuckwit.\n",
      "Motherfucking\n",
      "Motherfucker.+Example:\n",
      "Motherfucking\n",
      "fuckstick.\n",
      "fuck.\n",
      "motherfucker.\n",
      "fuck,\n",
      "\"\"fuck\n",
      "\"\"fucking\"\"\n",
      "\"\"fucking\"\"\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "fuck.\n",
      "lord/fuck\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "fucker!!!\n",
      "what-so-fucking-ever!!!\n",
      "dickhead....fuck\n",
      "dickhead....fuck\n",
      "dickhead....fuck\n",
      "dickhead....fuck\n",
      "dickhead....fuck\n",
      "dickhead....fuck\n",
      "fuckish\n",
      "fucker!!\n",
      "fuck?\n",
      "(fuck\n",
      "Motherfucking\n",
      "fucked.\n",
      "fuckin'\n",
      "dumbfuck+The\n",
      "fuck's\n",
      "fuck!!!\n",
      "{{unblock-auto|...}fuck\n",
      "\"\"fuck\n",
      "fuck.\n",
      "website.++fuck\n",
      "\"\"clusterfucked\n",
      "motherfucker.\n",
      "fuck-sucking\n",
      "julianfuckton?\n",
      "dog-fucking?\n",
      "examples!+http://propagandapress.org/2007/02/21/fuck-bubba-by-saab-lofton/+http://propagandapress.org/2007/02/26/man-fuck-florida-talking-shit-about-cuba-saab-lofton/+http://activistsinlasvegas.blogspot.com/2006/12/rules-for-whites-by-saab-lofton.html\n",
      "motherfucker.\n",
      "Dumbfuck\n",
      "motherfucker.\n",
      "fuck.\n",
      "fuck-head.\n",
      "fuck.\n",
      "talk:Larryfuckstylinson\n",
      "muthafuckaa+How\n",
      "drugged/fucked\n",
      "fuckin'\n",
      "fuck?\n",
      "fucker.\n",
      "fuck,\n",
      "fucka..what\n",
      "bitch.....++fuck\n",
      "buhay..,fuck\n",
      "fuckwit.\n",
      "fucker.\n",
      "fucker!!!\n",
      "http://gawker.com/5636765/facebook-ceo-admits-to-calling-users-dumb-fucks+\n",
      "http://www.theweek.co.uk/technology/14625/are-users-%E2%80%98dumb-fucks%E2%80%99-trusting-data-facebook+\n",
      "http://tdh.me/zuckerberg-called-early-facebook-users-dumb-fucks-so-what/+\n",
      "http://anphicle.com/en/they-trust-me-dumb-fucks-facebook-ceo-mark-zuckerberg/+\n",
      "fucks\"\".\n",
      "fucks.++In\n",
      "fucka.+So\n",
      "fucka....\n",
      "fucking!\n",
      "fuck.\n",
      "fucks,\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "\"\"fuck\n",
      "++fuckoff\n",
      "Motherfucked++you\n",
      "me++Motherfucker++Ugh\"\n",
      "motherfucker,\n",
      "fuck.\n",
      "fucker!\n",
      "pigfucks,\n",
      "fuck.\n",
      "motherfucker.\n",
      "\\fuck\n",
      "fucked.\n",
      "fucker.\"\n",
      "\"\"fuck\"\"\n",
      "fuckers!!!!\n",
      "fuckin'\n",
      "motherfucker,\n",
      "fuck!\n",
      "fucker.\n",
      "fuck's\n",
      "\"\"clusterfucked\n",
      "\"\"clusterfuck\"\"\n",
      "Ratfucking\n",
      "\"\"fuck\"\"?\n",
      "\"\"fuck\n",
      "\"\"fucking\"\"\n",
      "fuck!\n",
      "++fuck\n",
      "fucks,\n",
      "bullshit+fuck\n",
      "motherfucker.\n",
      "\"\"fucking\"\"\n",
      "\"\"fucking\"\"\n",
      "mo'fucka!!\"\n",
      "'fuck'\n",
      "'fuck'\n",
      "\"\"fuck\"\"\n",
      "fuck(that\n",
      "fuckwitz\n",
      "fuckitz\n",
      "fucker????\n",
      "fuck:++It\n",
      "++Motherfucker\n",
      "motherfucker.\n",
      "\"\"fuck\n",
      "motherfucker.\n",
      "fuck,\n",
      "Congrad-a-fucking-lations\n",
      "fuck,\n",
      "dumbfuck.\n",
      "http://gawker.com/5636765/facebook-ceo-admits-to-calling-users-dumb-fucks+\n",
      "http://www.theweek.co.uk/technology/14625/are-users-%E2%80%98dumb-fucks%E2%80%99-trusting-data-facebook+\n",
      "http://tdh.me/zuckerberg-called-early-facebook-users-dumb-fucks-so-what/+\n",
      "http://anphicle.com/en/they-trust-me-dumb-fucks-facebook-ceo-mark-zuckerberg/+\n",
      "fucks\"\".\n",
      "fucks.++In\n",
      "fuck,\n",
      "fucks,\n",
      "++fuck\n",
      "motherfucker,\n",
      "motherfuck.\n",
      "textfuck\n",
      "motherfucker,even\n",
      "++fuck\n",
      "motherfucker.\n",
      "fuck,\n",
      "'fucking'.\n",
      "fuck.\n",
      "(fucking)\n",
      "weeds.\"\"+fuck\n"
     ]
    }
   ],
   "source": [
    "for r in X_train:\n",
    "    words = r.split()\n",
    "    for w in words:\n",
    "        if w not in word_to_index:\n",
    "            if 'fuck' in w:\n",
    "                print(w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is naive model based on averaging all word embeddings in a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_comment(comment, word_to_vec):\n",
    "    \"\"\"\n",
    "    Converts a comment into a list of words. \n",
    "    Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    comment -- string\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary \n",
    "    into its n-dimensional vector representation\n",
    "    \n",
    "    Returns:\n",
    "    avg -- average vector encoding information about the sentence, numpy-array of shape (n,)\n",
    "    \"\"\"\n",
    "    words = comment[0].lower().split()\n",
    "    avg = np.zeros(shape=(len(word_to_vec['apple']), ))\n",
    "    for w in words:\n",
    "        if w in word_to_vec:\n",
    "            avg += word_to_vec[w]\n",
    "    avg = avg / len(words)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = list()\n",
    "for c in X_train:\n",
    "    averages.append(average_comment(c, word_to_vec))\n",
    "averages = np.asarray(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mazhurin/keras/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "averages_test = list()\n",
    "for c in X_test:\n",
    "    averages_test.append(average_comment(c, word_to_vec))\n",
    "averages_test = np.asarray(averages_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from keras.backend import int_shape\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "_EPSILON = K.epsilon()\n",
    "    \n",
    "def custom_loss(y_true, y_pred):\n",
    "    losses = []\n",
    "    y_pred = K.clip(y_pred, _EPSILON, 1.0-_EPSILON)\n",
    "    num_classes = int_shape(y_pred)[1]\n",
    "    for i in range(num_classes):\n",
    "        losses.append(K.mean(-(\n",
    "            y_true[:, i]*K.log(y_pred[:, i]) - (1-y_true[:, i])*K.log(1-y_pred[:, i])\n",
    "        ), axis = -1))\n",
    "    loss = tf.stack(losses)\n",
    "    loss = K.mean(loss, axis=-1)\n",
    "        \n",
    "    return loss\n",
    "    #return tf.convert_to_tensor(losses, dtype='float32')\n",
    "\n",
    "def baseline_model(num_dim=25, num_labels=6):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_labels, input_shape=(num_dim,), kernel_initializer='normal', \n",
    "                    activation='sigmoid'))\n",
    "    #model.add(Activation('sigmoid'))\n",
    "    model.compile(\n",
    "        #loss='binary_crossentropy', \n",
    "        loss=custom_loss,\n",
    "        optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 35008/159571 [=====>........................] - ETA: 12s - loss: -7.2864 - acc: 0.0599"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-a36c5d050f16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2475\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(averages, Y_train, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(averages[3003:3005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[3003:3005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "estimator_baseline = KerasRegressor(build_fn=baseline_model, \n",
    "                                    nb_epoch=1, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 42035/127656 [========>.....................] - ETA: 36s - loss: -14.1116 - acc: 0.8558"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-5b653b2bace7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results: %.2f (%.2f) MSE\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = cross_val_score(estimator_baseline, averages, Y_train, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999994 , 0.99999964, 0.9999995 , 0.99999964, 0.9999995 ,\n",
       "        0.99999964]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.asarray([average_comment(\"Love You\", word_to_vec)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    X_indices = np.zeros(shape=(m, max_len))\n",
    "    for i in range(m):                               \n",
    "        sentence_words = X[i].lower().split()\n",
    "        if len(sentence_words) > max_len:\n",
    "            print (\"Too many words:\", len(sentence_words))\n",
    "            continue\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            else :\n",
    "                X_indices[i, j] = word_to_index['word']\n",
    "            j = j + 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many words: 2015\n",
      "Too many words: 2321\n",
      "Too many words: 1667\n",
      "Too many words: 1667\n"
     ]
    }
   ],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, max_comment)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe \n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_len = len(word_to_index) + 1                  \n",
    "    emb_dim = word_to_vec[\"cucumber\"].shape[0]  \n",
    "    emb_matrix = np.zeros(shape=(vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(input_shape, num_classes, word_to_vec, word_to_index):\n",
    "    \"\"\"\n",
    "    Create the model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec -- dictionary mapping every word in a vocabulary into its n-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary \n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    sentence_indices = Input(shape=input_shape, dtype='float32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    X = Bidirectional(LSTM(18, return_sequences = False))(embeddings)\n",
    "    X = Dropout(rate=0.1)(X)\n",
    "#     X = LSTM(8, return_sequences = False)(X)\n",
    "#     X = Dropout(rate=0.2)(X)\n",
    "    X = Dense(units=50, activation='relu')(X)\n",
    "    X = Dropout(0.1)(X)\n",
    "    X = Dense(units=num_classes, activation='sigmoid')(X)\n",
    "    model = Model(inputs = [sentence_indices], outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1411)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1411, 25)          29837875  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 36)                6336      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                1850      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 29,846,367\n",
      "Trainable params: 8,492\n",
      "Non-trainable params: 29,837,875\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_RNN = rnn_model((max_comment,), num_classes, word_to_vec, word_to_index)\n",
    "model_RNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "159571/159571 [==============================] - 1968s 12ms/step - loss: 0.1147 - acc: 0.9615\n",
      "Epoch 2/3\n",
      "159571/159571 [==============================] - 2012s 13ms/step - loss: 0.0806 - acc: 0.9736\n",
      "Epoch 3/3\n",
      "  8832/159571 [>.............................] - ETA: 37:44 - loss: 0.0722 - acc: 0.9754"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b9e9bb176d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_RNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_RNN.fit(X_train_indices, Y_train, epochs = 2, batch_size = 128, shuffle=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(probabilities, title):\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    fig.suptitle(title)\n",
    "    plt.autoscale(enable=False, axis='y')\n",
    "    width = 0.85\n",
    "    ind = np.arange(len(probabilities))\n",
    "    print(probabilities)\n",
    "    plt.bar(ind, probabilities, width=width)\n",
    "    plt.xticks(ind, labels)\n",
    "    \n",
    "def predict(text, model):\n",
    "    plot_predictions(model.predict(sentences_to_indices(\n",
    "        np.asarray([text]),word_to_index, max_comment))[0], text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60460013 0.03970927 0.37945572 0.02859204 0.3288621  0.0551744 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29edd63c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(\"I hate you\", model_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1509509  0.00603852 0.07105798 0.0083426  0.06707279 0.00881005]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAEWCAYAAABlvlEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAG59JREFUeJzt3Xu8XGV97/HPl4Ro5WolIhIQjgYx1VPULd6PUdFy0aAHFKiKopbTFrBeaIutIodaC/V6RLygIqjlJmobAQ8CQkUUSUDlEkRTiBJECIgooCL66x/r2WSy2Tt7SGZn75183q/XvLLWmmfW/GatzFrfefazZlJVSJIkSRu6jSa7AEmSJGkqMBhLkiRJGIwlSZIkwGAsSZIkAQZjSZIkCTAYS5IkSYDBWNIUl+SoJJ+f7DrWN0mWJdltLR5fSR7Xpj+e5J2Dq06SJsfMyS5AkpIsA95YVedPdi168KrqLye7BkkaBHuMJUl9S2KHiqT1lsFY0rSSZEGSa5L8IslFSZ7Qlv99kjNHtP1/ST7cprdI8ukkNye5Kcm7k8wYZf2PSnJPkkf0LHtKkhVJNk6yUZJ3JPlxkluTfDbJFq3d/CTLR6yv7yELre3fJrkyyd2t3q2TfDXJr5Kcn+Thre3ZSQ4b8fgrk7x8jHW/ptV8e5J/HHHfrkm+3bbpzUk+kmRWz/2V5JAkPwJ+NMq6T0ry7t5tkORtbfvcnOSgnraPSPKVJL9Msqjth2/2s30kaaIZjCVNG0l2Ak4F3gzMBs4BvtJC3GnAnkk2a21nAK8ETmkPPwm4D3gc8GTgxcAbRz5HVf0MuKg9dthrgNOq6nfA69rt+cD/ADYFPjKwFwn7AC8CdgJeCnwV+Ae617sR8KbW7mTg1cMPSvKnwLbA2SNXmGQe8LH2Oh4NPAKY09Pk98BbgK2AZwIvBP56xGpeBjwdmNfHa3gUsEWr5w3A8cOBHjgeuLu1eW27SdKUYDCWNJ3sB5xdVee1kPo+4I+AZ1XVj4ErgOEe0xcA91TVpUm2BvYE3lxVd1fVrcAHgf3HeJ77Q2cL2AcAn2v3vQr4QFVdX1V3AW8H9h/gEIPjquqWqroJuBj4TlV9t6p+A3yZLtQDLAR2SjK3zb8GOL2q7h1lnfsCZ1XVN6rqt8A7gT8M31lVl1fVpVV1X1UtAz4BPG/EOv6lqn5eVb/u4zX8Dji6qn5XVecAdwGPb9tyH+BdVXVPVS2h29aSNCUYjCVNJ48Gfjw8U1V/AG6k65mErnf4gDb956zsLX4MsDFwcxsu8Au68PfIMZ7nP4B5SXak6729s6ouG62GNj0T2HotXlevW3qmfz3K/KYALSifDrw6yUasGt5HejTddqI99m7g9uH5JDslOSvJz5L8EngPXe9xrxvp3+1VdV/P/D2t7tl026p3XQ9mvZI0oQzGkqaTn9KFXACSBNgOuKkt+gIwP8kcup7j4WB8I/BbYKuq2rLdNq+qPxntSVroPIOu1/g1rBo4V6kB2J5uiMYtdEMEHtZT3wy6MDhRTqbrwX4hXe/4t8dodzPddhqu62F0wymGfQz4ATC3qjanG7qREeuoAdS7gm5b9Q7j2G6MtpK0zhmMJU0nZwB7JXlhko2Bt9EF3m8BVNUKuvHBnwFuqKpr2/Kbga8B70+yebuA7rFJRg4X6PVZurHEC1g1GJ8KvCXJjkk2petdPb31kP4QeGiSvVp97wAeMvzAdmHaIAIm7XV9m25IxPsZu7cY4EzgJUme08ZjH82qx//NgF8CdyXZGfirQdU4ot7fA18CjkrysPZcB07Ec0nSmjAYS5o2quo6ul7c44Db6C5Oe+mIcbWnALuxsrd42IHALGAJcAddWNxmNc91CV3ovKKNXx52Il0I/QZwA/Ab4LD2mDvpLlr7FF0v9t1A77dUbEcL8QP0WeBJwJg/glJV1wCH0G2Tm+lef29dh9MNPfkV8Em6IRoT5VC6C/N+RrcdT6X7cCNJky5VA+u8kKT1SpKvA6dU1acGtL5PAV+oqnMHsb62zgOBg6vqOYNa57qU5FjgUVXlt1NImnQGY0kaRZKnAecB21XVrya7ntG0scJfBz5aVZ+d7Hr60YZPzAKuAp5G95V7b6yqf5/UwiQJh1JI0gMkORk4n+7r3aZqKP4zuovZbuGBw0amss3oxhnfTTdk4/103wIiSZPOHmNJkiQJe4wlSZIkwGAsSZIkAQZjSZIkCTAYS5IkSYDBWJIkSQIMxpIkSRJgMJYkSZIAg7EkSZIEGIwlSZIkwGAsSZIkAQZjSZIkCegjGCc5McmtSa4e4/4k+XCSpUmuTPKUwZcpSZIkTax+eoxPAnZfzf17AHPb7WDgY2tfliRJkrRujRuMq+obwM9X02Rv4LPVuRTYMsk2gypQkiRJWhdmDmAd2wI39swvb8tuHtkwycF0vcpssskmT915550H8PSSJEnS2C6//PLbqmr2eO0GEYz7VlUnACcADA0N1eLFi9fl00uSJGkDlOTH/bQbxLdS3ARs1zM/py2TJEmSpo1BBOOFwIHt2ymeAdxZVQ8YRiFJkiRNZeMOpUhyKjAf2CrJcuBdwMYAVfVx4BxgT2ApcA9w0EQVK0mSJE2UcYNxVR0wzv0FHDKwiiRJkqRJ4C/fSZIkSRiMJUmSJMBgLEmSJAEGY0mSJAkwGEuSJEmAwViSJEkCDMaSJEkSYDCWJEmSAIOxJEmSBBiMJUmSJMBgLEmSJAEGY0mSJAkwGEuSJEmAwViSJEkCDMaSJEkSYDCWJEmSAIOxJEmSBBiMJUmSJMBgLEmSJAEGY0mSJAkwGEuSJEmAwViSJEkCDMaSJEkSYDCWJEmSAIOxJEmSBBiMJUmSJMBgLEmSJAEGY0mSJAkwGEuSJEmAwViSJEkCDMaSJEkSYDCWJEmSAIOxJEmSBBiMJUmSJKDPYJxk9yTXJVma5IhR7t8+yYVJvpvkyiR7Dr5USZIkaeKMG4yTzACOB/YA5gEHJJk3otk7gDOq6snA/sBHB12oJEmSNJH66THeFVhaVddX1b3AacDeI9oUsHmb3gL46eBKlCRJkibezD7abAvc2DO/HHj6iDZHAV9LchiwCbDbQKqTJEmS1pFBXXx3AHBSVc0B9gQ+l+QB605ycJLFSRavWLFiQE8tSZIkrb1+gvFNwHY983Pasl5vAM4AqKpvAw8Fthq5oqo6oaqGqmpo9uzZa1axJEmSNAH6CcaLgLlJdkwyi+7iuoUj2vwEeCFAkifQBWO7hCVJkjRtjBuMq+o+4FDgXOBaum+fuCbJ0UkWtGZvA/4iyfeBU4HXVVVNVNGSJEnSoPVz8R1VdQ5wzohlR/ZMLwGePdjSJEmSpHXHX76TJEmSMBhLkiRJgMFYkiRJAgzGkiRJEmAwliRJkgCDsSRJkgQYjCVJkiTAYCxJkiQBBmNJkiQJMBhLkiRJgMFYkiRJAgzGkiRJEmAwliRJkgCDsSRJkgQYjCVJkiTAYCxJkiQBBmNJkiQJMBhLkiRJgMFYkiRJAgzGkiRJEmAwliRJkgCDsSRJkgQYjCVJkiTAYCxJkiQBBmNJkiQJMBhLkiRJgMFYkiRJAgzGkiRJEmAwliRJkgCDsSRJkgQYjCVJkiTAYCxJkiQBBmNJkiQJMBhLkiRJgMFYkiRJAvoMxkl2T3JdkqVJjhijzSuTLElyTZJTBlumJEmSNLFmjtcgyQzgeOBFwHJgUZKFVbWkp81c4O3As6vqjiSPnKiCJUmSpInQT4/xrsDSqrq+qu4FTgP2HtHmL4Djq+oOgKq6dbBlSpIkSROrn2C8LXBjz/zytqzXTsBOSS5JcmmS3UdbUZKDkyxOsnjFihVrVrEkSZI0AQZ18d1MYC4wHzgA+GSSLUc2qqoTqmqoqoZmz549oKeWJEmS1l4/wfgmYLue+TltWa/lwMKq+l1V3QD8kC4oS5IkSdNCP8F4ETA3yY5JZgH7AwtHtPl3ut5ikmxFN7Ti+gHWKUmSJE2ocYNxVd0HHAqcC1wLnFFV1yQ5OsmC1uxc4PYkS4ALgb+tqtsnqmhJkiRp0FJVk/LEQ0NDtXjx4kl5bkmSJG04klxeVUPjtfOX7yRJkiQMxpIkSRJgMJYkSZIAg7EkSZIEGIwlSZIkwGAsSZIkAQZjSZIkCTAYS5IkSYDBWJIkSQIMxpIkSRJgMJYkSZIAg7EkSZIEGIwlSZIkwGAsSZIkAQZjSZIkCTAYS5IkSYDBWJIkSQIMxpIkSRJgMJYkSZIAg7EkSZIEGIwlSZIkwGAsSZIkAQZjSZIkCTAYS5IkSYDBWJIkSQIMxpIkSRJgMJYkSZIAg7EkSZIEGIwlSZIkwGAsSZIkAQZjSZIkCTAYS5IkSYDBWJIkSQIMxpIkSRLQZzBOsnuS65IsTXLEatrtk6SSDA2uREmSJGnijRuMk8wAjgf2AOYBBySZN0q7zYC/Ab4z6CIlSZKkidZPj/GuwNKqur6q7gVOA/Yepd0/AccCvxlgfZIkSdI60U8w3ha4sWd+eVt2vyRPAbarqrMHWJskSZK0zqz1xXdJNgI+ALytj7YHJ1mcZPGKFSvW9qklSZKkgeknGN8EbNczP6ctG7YZ8ETgoiTLgGcAC0e7AK+qTqiqoaoamj179ppXLUmSJA1YP8F4ETA3yY5JZgH7AwuH76yqO6tqq6raoap2AC4FFlTV4gmpWJIkSZoA4wbjqroPOBQ4F7gWOKOqrklydJIFE12gJEmStC7M7KdRVZ0DnDNi2ZFjtJ2/9mVJkiRJ65a/fCdJkiRhMJYkSZIAg7EkSZIEGIwlSZIkwGAsSZIkAQZjSZIkCTAYS5IkSYDBWJIkSQIMxpIkSRJgMJYkSZIAg7EkSZIEGIwlSZIkwGAsSZIkAQZjSZIkCTAYS5IkSYDBWJIkSQIMxpIkSRJgMJYkSZIAg7EkSZIEGIwlSZIkwGAsSZIkAQZjSZIkCTAYS5IkSYDBWJIkSQIMxpIkSRJgMJYkSZIAg7EkSZIEGIwlSZIkwGAsSZIkAQZjSZIkCTAYS5IkSYDBWJIkSQIMxpIkSRJgMJYkSZIAg7EkSZIE9BmMk+ye5LokS5McMcr9b02yJMmVSS5I8pjBlypJkiRNnHGDcZIZwPHAHsA84IAk80Y0+y4wVFX/EzgT+NdBFypJkiRNpH56jHcFllbV9VV1L3AasHdvg6q6sKruabOXAnMGW6YkSZI0sfoJxtsCN/bML2/LxvIG4Kuj3ZHk4CSLkyxesWJF/1VKkiRJE2ygF98leTUwBLx3tPur6oSqGqqqodmzZw/yqSVJkqS1MrOPNjcB2/XMz2nLVpFkN+AfgedV1W8HU54kSZK0bvTTY7wImJtkxySzgP2Bhb0NkjwZ+ASwoKpuHXyZkiRJ0sQaNxhX1X3AocC5wLXAGVV1TZKjkyxozd4LbAp8Icn3kiwcY3WSJEnSlNTPUAqq6hzgnBHLjuyZ3m3AdUmSJEnrlL98J0mSJGEwliRJkgCDsSRJkgQYjCVJkiTAYCxJkiQBfX4rxfpmhyPOnuwS1mvLjtlrskuQJEl60OwxliRJkjAYS5IkSYDBWJIkSQIMxpIkSRJgMJYkSZIAg7EkSZIEGIwlSZIkwGAsSZIkAQZjSZIkCTAYS5IkSYDBWJIkSQIMxpIkSRJgMJYkSZIAg7EkSZIEGIwlSZIkwGAsSZIkATBzsguQNL3scMTZk13Cem3ZMXtNdgmStMEyGEuStI74wXJi+cFSa8uhFJIkSRIGY0mSJAkwGEuSJEmAwViSJEkCDMaSJEkSYDCWJEmSAIOxJEmSBBiMJUmSJMBgLEmSJAEGY0mSJAnoMxgn2T3JdUmWJjlilPsfkuT0dv93kuww6EIlSZKkiTRuME4yAzge2AOYBxyQZN6IZm8A7qiqxwEfBI4ddKGSJEnSRJrZR5tdgaVVdT1AktOAvYElPW32Bo5q02cCH0mSqqoB1ipJGscOR5w92SWs15Yds9dkl6AJ5PtnYk2H908/wXhb4Mae+eXA08dqU1X3JbkTeARw2yCK1IbFA9PEmg4HJkmSJkM/wXhgkhwMHNxm70py3bp8/mlsK6bRh4xseANp3D9Tm/tnanP/TG3un6nN/dO/x/TTqJ9gfBOwXc/8nLZstDbLk8wEtgBuH7miqjoBOKGfwrRSksVVNTTZdWh07p+pzf0ztbl/pjb3z9Tm/hm8fr6VYhEwN8mOSWYB+wMLR7RZCLy2Te8LfN3xxZIkSZpOxu0xbmOGDwXOBWYAJ1bVNUmOBhZX1ULg08DnkiwFfk4XniVJkqRpo68xxlV1DnDOiGVH9kz/BnjFYEtTD4efTG3un6nN/TO1uX+mNvfP1Ob+GbA44kGSJEnyJ6ElSZIkwGA8KZJsmeSv1/CxQ0k+POiapKksyQ5Jrp7sOjS63mNakvlJzpqg55mf5FkTse4NQZJvDXh9978vk+ySZM9Brl+aDAbjybElsEbBuKoWV9WbBlyPJsjansiTHJ1kt0HWJE2AB31MSzJjDZ5nPmAwXkNVNZHbbhdggwvGY33YSHJSkn3XcJ2rfMhIsiDJEW36ZUnmreF6lyXZak3r2FAYjCfHMcBjk3wvyXvb7eokVyXZDyDJy5NckM42SX6Y5FG9vTFJNk3ymfa4K5PsM6mvagPQvqf7wZjPWpzIq+rIqjp/TR8/XSV5a3tPXJ3kzW3xzCT/luTaJGcmeVhre0ySJe098L62bOskX07y/XZ7Vlv+6iSXtffeJ4bDWZK7kvxza3tpkq3b8tlJvphkUbs9exI2x3Rw/zENeC+wadtHP2j7LHD/ifnYJFcAr0jy2CT/P8nlSS5OsnNr99Ik30ny3STnt/25A/CXwFva/nvu5LzU6SvJXe3f+UkuGmMfjfZ+WiXkDa+nZ34WcDSwX9s3+627VzW5JujDxiofMqpqYVUd02ZfBqxRMF7bOjYYVeVtHd+AHYCr2/Q+wHl0X4W3NfATYJt23+eBQ4GzgAPasvnAWW36WOBDPet9+GS/tkncppsAZwPfB64G9gOeCvwncDnd1w1uA+wMXDZiX1zVph/Qvi2/CPgQsBh4GzAb+CLdd3wvAp69mv38M7ofwPke8Ny27OvAlcAFwPat7X8AB7bp/wP8W5s+Cdi3TT8N+FZ7jZcBm032dp+gfflU4Kq2TzcFrgGeDNTwtgZOBA6n++n561h5IfGW7d/TgTe36Rl0Pzr0BOArwMZt+Ud7tnkBL23T/wq8o02fAjynTW8PXDvZ22cq3lj1mDYfuJPux6A2Ar7dsw2XAX/X87gLgLlt+ul034EP8PCeffpG4P1t+ijg8Ml+vdP1Bty1un20mvfT/cehEevp3e+vAz4y2a9xErdpgI+07Xc+3Td5DR+7V3duObYdz39Id46YRZcDVtCdN/Yb3rZ0nSw/B25o9z0WuKKnlrm986PUugz4v8AVdMfYndvyXdv/ge/SnWMeP0Ydm9Adey9rbfee7O0/Ebd1+pPQGtVzgFOr6vfALUn+ky4ALQQOowt5l1bVqaM8djd6vjO6qu5YB/VOVbsDP62qvQCSbAF8le6Nu6L1YPxzVb0+yawkO1bVDXRv9tOTbAwcN7I98Pq2/lnVfl0oySnAB6vqm0m2pzvQPWFkQVW1LMnH6Q6cwz0vXwFOrqqTk7we+DBdD8DBwCVJbqAL38/oXVfrkTkd2K+qFiXZHPj1gLbdVPMc4MtVdTdAki/RnTBurKpLWpvPA2+i+8DyG+DT7S8pw2NbXwAcCNDeW3cmeQ3dCWpR6xz7I+DW1v7ensdeDryoTe8GzGvtATZPsmlVrdJjpge4rKqWA7Re5B2Ab7b7Tm/LN6U70X+hZ/s+pP07h+59uQ3dCfqGdVP2BmW0fXQpo7+fNL6X0wXKeXSdXEuAE/s4t8ysql3bkIV3VdVuSY4EhqrqUIAkrwOoqm8lWUjXOXZmu+/OJLtU1feAg4DPjFPnbVX1lHTXBBxO98HzB8Bzq/vdit2A91TVPqPU8R66D6+vT7IlcFmS84eP1esLg/HUNgf4A7B1ko2q6g+TXdAUdhXw/iTH0h3M7wCeCJzXTrozgJtb2zPoAvEx7d/96A5oY7WHdjJv1iYsPRP43236c3S9k1TVLe0gdCHw8qr6+YjHPR64uaoWtfa/7OO51jcjv1uy2oF8V+CFdL+6eShdKB5N6D6UvH2U+35XresE+D0rj40bAc+o7rva1b/f9kz3bk+A4ZPoRsAvqmqXUR5/HPCBqlqYZD5dT7EG6wH7aDXvp/toQy+TbET3YUWr+l+s7OT6aZKvt+XjnVu+1P69nO7DyYP1KeCgJG+lO5ftOk773ucbPhdtAZycZC7dcXbjMR77YmBBksPb/ENpf0lbg7qnLMcYT45fAZu16YvpxmXNSDKb7s11WbqxrCcCB9D9p3vrKOs5DzhkeCbJwye06imsqn4IPIUuIL+bbojKNVW1S7s9qape3JqfDrwyyU7dQ+tHdKFprPaw8mQOK8PScNttB9SD+CTgduDRA1jXdHYx8LIkD0uyCV1PzMXA9kme2dr8OfDN1uu4RXU/QvQW4E/b/RcAfwXdRV7tLwgXAPsmeWRb/sdJHjNOLV+j+8sN7TGjhTitekzrS/twd0OSVwCkM7z/tqAbggTw2rV5HvVvNe+nZXR/bQFYwOjByX0zuvHOLcMfUEZ+gOzXF4E9gJcAl1fV7eO0H+35/gm4sKqeCLyULvCOJsA+Pa9l+6par0IxGIwnRfuPe0m6r7l5Jt140+/TjT39u6r6GfAPwMVV9U26UPzGJCP/XP9u4OHpLlD6PvD8dfYippgkjwbuqarP013883Rg9nCQSrJxkj8BqKr/ojsovJOVPcHXjdV+FA8mLI08WXyLlcNfXkUX+Gi9NHvQjaU9PMmOI9ZzHbBNkqe19pvlwV8IOC1U1RV0YxovA75D1yNyB902OCTJtXRjUD9Gt23PSnIl3Z/qhz9A/g3w/CRX0fWMzKuqJcA7gK+19ufRjTtfnTcBQ+1CpCV0F39phBHHtPc+iIe+CnhDO35dA+zdlh9FN8TicuC2nvZfAV4eL76bKGO9nz4JPK/tp2eyakfBsAvp/pK2QV181+MbrOzk2oaV5+MHc24ZtroPGavc1/6adS7d8XC8YRRj6f0g+rrV1HEucFhy/4WaT17D55vS/OU7rReS/BndCfkPwO/oegvvoxvDuwXdJ+MPVdUnW/vDW/sdq2pZW7bLaO2TXER3wc/i1m4r4Hi6ccUzgW9U1aiBqfVKn9nqOozuYobPAFvRXdRwEHALXQg8qKquSLKArrfmBa3tWVV1ZgvFx9GNjf01sJtjXSVp8iS5q6o2bWHxOLrrE35Cdx46sR27xz23tPPK4qraIckf04XQjYF/oTvmD1XVoem+GeeTdD2/+1bVfyV5Bt155jFtKMdYtS5r67ktyRDwvqqa30L7yXQfeM4GXj1GHQvprut4Fl3H6g1V9ZIBbcopw2AsSZI0TbWOni2q6p2TXcv6YL38U6wkSdL6LsmX6b62bayLjvUg2WMsDUCSg+jGtfa6pKoOGa29JEkToYXlkdep/H1VnTsZ9Uw3BmNJkiQJv5VCkiRJAgzGkiRJEmAwliRJkgCDsSRJkgQYjCVJkiQA/htfnYBXS2hw0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29ef55ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(\"I love you, my darling\", model_RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model_RNN.predict(X_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([\n",
    "    pd.DataFrame(id_test, columns=['id']), \n",
    "    pd.DataFrame(y, columns=labels)], axis=1)\n",
    "result = result.set_index('id')\n",
    "result = result.fillna(0.0)\n",
    "result.to_csv('./result/anton_rnn_{}.csv'.format(datetime.now()), header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001cee341fdb12</th>\n",
       "      <td>0.924555</td>\n",
       "      <td>0.299127</td>\n",
       "      <td>0.809626</td>\n",
       "      <td>0.057377</td>\n",
       "      <td>0.741484</td>\n",
       "      <td>0.170765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000247867823ef7</th>\n",
       "      <td>0.011904</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00013b17ad220c46</th>\n",
       "      <td>0.024313</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.011640</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.009421</td>\n",
       "      <td>0.001319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017563c3f7919a</th>\n",
       "      <td>0.010370</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017695ad8997eb</th>\n",
       "      <td>0.010542</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.000274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "id                                                                       \n",
       "00001cee341fdb12  0.924555      0.299127  0.809626  0.057377  0.741484   \n",
       "0000247867823ef7  0.011904      0.000080  0.002779  0.000097  0.002498   \n",
       "00013b17ad220c46  0.024313      0.001054  0.011640  0.000894  0.009421   \n",
       "00017563c3f7919a  0.010370      0.000064  0.002445  0.000077  0.002208   \n",
       "00017695ad8997eb  0.010542      0.000126  0.003123  0.000148  0.002547   \n",
       "\n",
       "                  identity_hate  \n",
       "id                               \n",
       "00001cee341fdb12       0.170765  \n",
       "0000247867823ef7       0.000266  \n",
       "00013b17ad220c46       0.001319  \n",
       "00017563c3f7919a       0.000189  \n",
       "00017695ad8997eb       0.000274  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>0.999977</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.999987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0002eadc3b301559</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0003806b11932181</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>000634272d0d44eb</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>000663aff0fffc80</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>000689dd34e20979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>000834769115370c</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>000844b52dee5f3f</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>00084da5d4ead7aa</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>00091c35fa9d0465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>000968ce11f5ee34</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0009734200a85047</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>00097b6214686db5</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0009aef4bd9e1697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>000a02d807ae0254</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>000a6c6d4e89b9bc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>000bafe2080bba82</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>000bf0a9894b2807</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153134</th>\n",
       "      <td>153134</td>\n",
       "      <td>fff3ae2e177b6bb3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153135</th>\n",
       "      <td>153135</td>\n",
       "      <td>fff4109e837f7acc</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153136</th>\n",
       "      <td>153136</td>\n",
       "      <td>fff4373a81ef9f2a</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153137</th>\n",
       "      <td>153137</td>\n",
       "      <td>fff460574ddbcd80</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153138</th>\n",
       "      <td>153138</td>\n",
       "      <td>fff4fc0a1555be5c</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153139</th>\n",
       "      <td>153139</td>\n",
       "      <td>fff5b9bb944d634c</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153140</th>\n",
       "      <td>153140</td>\n",
       "      <td>fff5c4a77fe0c05f</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153141</th>\n",
       "      <td>153141</td>\n",
       "      <td>fff5fb61bd637c82</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153142</th>\n",
       "      <td>153142</td>\n",
       "      <td>fff69311f306df44</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153143</th>\n",
       "      <td>153143</td>\n",
       "      <td>fff6ad63666fb304</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153144</th>\n",
       "      <td>153144</td>\n",
       "      <td>fff7159b3ee95618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153145</th>\n",
       "      <td>153145</td>\n",
       "      <td>fff718ffe5f05559</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153146</th>\n",
       "      <td>153146</td>\n",
       "      <td>fff7fc22a0cdccd3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153147</th>\n",
       "      <td>153147</td>\n",
       "      <td>fff83b80284d8440</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153148</th>\n",
       "      <td>153148</td>\n",
       "      <td>fff8ef316d0c6990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153149</th>\n",
       "      <td>153149</td>\n",
       "      <td>fff8f521a7dbcd47</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153150</th>\n",
       "      <td>153150</td>\n",
       "      <td>fff8f64043129fa2</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153151</th>\n",
       "      <td>153151</td>\n",
       "      <td>fff9d70fe0722906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153152</th>\n",
       "      <td>153152</td>\n",
       "      <td>fff9fa508f400ee6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153153</th>\n",
       "      <td>153153</td>\n",
       "      <td>fffa3fae1890b40a</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153154</th>\n",
       "      <td>153154</td>\n",
       "      <td>fffa8a11c4378854</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153155</th>\n",
       "      <td>153155</td>\n",
       "      <td>fffac2a094c8e0e2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153156</th>\n",
       "      <td>153156</td>\n",
       "      <td>fffb5451268fb5ba</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153157</th>\n",
       "      <td>153157</td>\n",
       "      <td>fffc2b34bbe61c8d</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153158</th>\n",
       "      <td>153158</td>\n",
       "      <td>fffc489742ffe69b</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>153159</td>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>153160</td>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>153161</td>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>153162</td>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>153163</td>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                id     toxic  severe_toxic   obscene    threat  \\\n",
       "0            0  00001cee341fdb12  0.999972      0.999977  0.999979  0.999974   \n",
       "1            1  0000247867823ef7  1.000000      1.000000  1.000000  1.000000   \n",
       "2            2  00013b17ad220c46  1.000000      1.000000  1.000000  1.000000   \n",
       "3            3  00017563c3f7919a  0.999999      0.999999  0.999999  0.999999   \n",
       "4            4  00017695ad8997eb  0.999990      0.999990  0.999988  0.999991   \n",
       "5            5  0001ea8717f6de06  1.000000      1.000000  1.000000  1.000000   \n",
       "6            6  00024115d4cbde0f  0.999998      0.999999  0.999999  0.999999   \n",
       "7            7  000247e83dcc1211  0.999999      0.999999  0.999999  0.999999   \n",
       "8            8  00025358d4737918  1.000000      1.000000  1.000000  1.000000   \n",
       "9            9  00026d1092fe71cc  1.000000      1.000000  1.000000  1.000000   \n",
       "10          10  0002eadc3b301559  0.999990      0.999990  0.999988  0.999991   \n",
       "11          11  0002f87b16116a7f  1.000000      1.000000  1.000000  1.000000   \n",
       "12          12  0003806b11932181  1.000000      1.000000  1.000000  1.000000   \n",
       "13          13  0003e1cccfd5a40a  1.000000      1.000000  1.000000  1.000000   \n",
       "14          14  00059ace3e3e9a53  1.000000      1.000000  1.000000  1.000000   \n",
       "15          15  000634272d0d44eb  1.000000      1.000000  1.000000  1.000000   \n",
       "16          16  000663aff0fffc80  1.000000      1.000000  1.000000  1.000000   \n",
       "17          17  000689dd34e20979  1.000000      1.000000  1.000000  1.000000   \n",
       "18          18  000834769115370c  0.999999      0.999999  0.999999  0.999999   \n",
       "19          19  000844b52dee5f3f  0.999994      0.999995  0.999993  0.999994   \n",
       "20          20  00084da5d4ead7aa  1.000000      1.000000  1.000000  1.000000   \n",
       "21          21  00091c35fa9d0465  1.000000      1.000000  1.000000  1.000000   \n",
       "22          22  000968ce11f5ee34  0.999998      0.999999  0.999999  0.999999   \n",
       "23          23  0009734200a85047  1.000000      1.000000  1.000000  1.000000   \n",
       "24          24  00097b6214686db5  0.999999      0.999999  0.999999  0.999999   \n",
       "25          25  0009aef4bd9e1697  1.000000      1.000000  1.000000  1.000000   \n",
       "26          26  000a02d807ae0254  0.999995      0.999995  0.999994  0.999995   \n",
       "27          27  000a6c6d4e89b9bc  1.000000      1.000000  1.000000  1.000000   \n",
       "28          28  000bafe2080bba82  1.000000      1.000000  1.000000  1.000000   \n",
       "29          29  000bf0a9894b2807  0.999999      0.999999  0.999999  0.999999   \n",
       "...        ...               ...       ...           ...       ...       ...   \n",
       "153134  153134  fff3ae2e177b6bb3  1.000000      1.000000  1.000000  1.000000   \n",
       "153135  153135  fff4109e837f7acc  1.000000      1.000000  1.000000  1.000000   \n",
       "153136  153136  fff4373a81ef9f2a  1.000000      1.000000  1.000000  1.000000   \n",
       "153137  153137  fff460574ddbcd80  1.000000      1.000000  1.000000  1.000000   \n",
       "153138  153138  fff4fc0a1555be5c  0.999999      0.999999  0.999999  0.999999   \n",
       "153139  153139  fff5b9bb944d634c  0.999999      0.999999  0.999999  0.999999   \n",
       "153140  153140  fff5c4a77fe0c05f  1.000000      1.000000  1.000000  1.000000   \n",
       "153141  153141  fff5fb61bd637c82  0.999999      0.999999  0.999999  0.999999   \n",
       "153142  153142  fff69311f306df44  1.000000      1.000000  1.000000  1.000000   \n",
       "153143  153143  fff6ad63666fb304  0.999997      0.999997  0.999997  0.999998   \n",
       "153144  153144  fff7159b3ee95618  1.000000      1.000000  1.000000  1.000000   \n",
       "153145  153145  fff718ffe5f05559  0.999990      0.999990  0.999988  0.999991   \n",
       "153146  153146  fff7fc22a0cdccd3  1.000000      1.000000  1.000000  1.000000   \n",
       "153147  153147  fff83b80284d8440  0.999999      0.999999  0.999999  0.999999   \n",
       "153148  153148  fff8ef316d0c6990  1.000000      1.000000  1.000000  1.000000   \n",
       "153149  153149  fff8f521a7dbcd47  1.000000      1.000000  1.000000  1.000000   \n",
       "153150  153150  fff8f64043129fa2  0.999999      0.999999  0.999999  0.999999   \n",
       "153151  153151  fff9d70fe0722906  1.000000      1.000000  1.000000  1.000000   \n",
       "153152  153152  fff9fa508f400ee6  1.000000      1.000000  1.000000  1.000000   \n",
       "153153  153153  fffa3fae1890b40a  1.000000      1.000000  1.000000  1.000000   \n",
       "153154  153154  fffa8a11c4378854  1.000000      1.000000  1.000000  1.000000   \n",
       "153155  153155  fffac2a094c8e0e2  1.000000      1.000000  1.000000  1.000000   \n",
       "153156  153156  fffb5451268fb5ba  1.000000      1.000000  1.000000  1.000000   \n",
       "153157  153157  fffc2b34bbe61c8d  0.999999      0.999999  0.999999  0.999999   \n",
       "153158  153158  fffc489742ffe69b  1.000000      1.000000  1.000000  1.000000   \n",
       "153159  153159  fffcd0960ee309b5  1.000000      1.000000  1.000000  1.000000   \n",
       "153160  153160  fffd7a9a6eb32c16  1.000000      1.000000  1.000000  1.000000   \n",
       "153161  153161  fffda9e8d6fafa9e  1.000000      1.000000  1.000000  1.000000   \n",
       "153162  153162  fffe8f1340a79fc2  1.000000      1.000000  1.000000  1.000000   \n",
       "153163  153163  ffffce3fb183ee80  1.000000      1.000000  1.000000  1.000000   \n",
       "\n",
       "          insult  identity_hate  \n",
       "0       0.999985       0.999987  \n",
       "1       1.000000       1.000000  \n",
       "2       1.000000       1.000000  \n",
       "3       0.999999       0.999999  \n",
       "4       0.999987       0.999990  \n",
       "5       1.000000       1.000000  \n",
       "6       0.999999       0.999999  \n",
       "7       0.999999       0.999999  \n",
       "8       1.000000       1.000000  \n",
       "9       1.000000       1.000000  \n",
       "10      0.999987       0.999990  \n",
       "11      1.000000       1.000000  \n",
       "12      1.000000       1.000000  \n",
       "13      1.000000       1.000000  \n",
       "14      1.000000       1.000000  \n",
       "15      1.000000       1.000000  \n",
       "16      1.000000       1.000000  \n",
       "17      1.000000       1.000000  \n",
       "18      0.999999       0.999999  \n",
       "19      0.999994       0.999990  \n",
       "20      1.000000       1.000000  \n",
       "21      1.000000       1.000000  \n",
       "22      0.999999       0.999999  \n",
       "23      1.000000       1.000000  \n",
       "24      0.999999       0.999999  \n",
       "25      1.000000       1.000000  \n",
       "26      0.999994       0.999994  \n",
       "27      1.000000       1.000000  \n",
       "28      1.000000       1.000000  \n",
       "29      0.999999       0.999999  \n",
       "...          ...            ...  \n",
       "153134  1.000000       1.000000  \n",
       "153135  1.000000       1.000000  \n",
       "153136  1.000000       1.000000  \n",
       "153137  1.000000       1.000000  \n",
       "153138  0.999999       0.999999  \n",
       "153139  0.999999       0.999999  \n",
       "153140  1.000000       1.000000  \n",
       "153141  0.999999       0.999999  \n",
       "153142  1.000000       1.000000  \n",
       "153143  0.999998       0.999998  \n",
       "153144  1.000000       1.000000  \n",
       "153145  0.999987       0.999990  \n",
       "153146  1.000000       1.000000  \n",
       "153147  0.999999       0.999999  \n",
       "153148  1.000000       1.000000  \n",
       "153149  1.000000       1.000000  \n",
       "153150  0.999999       0.999999  \n",
       "153151  1.000000       1.000000  \n",
       "153152  1.000000       1.000000  \n",
       "153153  1.000000       1.000000  \n",
       "153154  1.000000       1.000000  \n",
       "153155  1.000000       1.000000  \n",
       "153156  1.000000       1.000000  \n",
       "153157  0.999999       0.999999  \n",
       "153158  1.000000       1.000000  \n",
       "153159  1.000000       1.000000  \n",
       "153160  1.000000       1.000000  \n",
       "153161  1.000000       1.000000  \n",
       "153162  1.000000       1.000000  \n",
       "153163  1.000000       1.000000  \n",
       "\n",
       "[153164 rows x 8 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(averages_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
