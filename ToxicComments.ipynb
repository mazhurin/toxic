{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of Toxic Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle competition: [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,GlobalMaxPool1D,Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_english_stopwords=stopwords.words('english')\n",
    "\n",
    "def data_clean(message):\n",
    "    temp_tw_list = word_tokenize(message)\n",
    "    # Remove stopwords\n",
    "    list_no_stopwords=[i for i in temp_tw_list if i.lower() not in     cache_english_stopwords]\n",
    "    # Remove hyperlinks\n",
    "    list_no_hyperlinks=[re.sub(r'https?:\\/\\/.*\\/\\w*','',i) for i in list_no_stopwords]\n",
    "    # Remove hashtags\n",
    "    list_no_hashtags=[re.sub(r'#', '', i) for i in list_no_hyperlinks]\n",
    "    #Remove numbers\n",
    "    list_no_numbers = [re.sub(r'$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$','', i) for i in list_no_hashtags]\n",
    "   # Remove Punctuation and split 's, 't, 've with a space for filter\n",
    "    list_no_punctuation=[re.sub(r'['+string.punctuation+']+', ' ', i) for i in list_no_numbers]\n",
    "    # Remove multiple whitespace\n",
    "    new_sent = ' '.join(list_no_punctuation)\n",
    "    # Remove any words with 2 or fewer letters\n",
    "    filtered_list = word_tokenize(new_sent)\n",
    "    list_filtered = [re.sub(r'^\\w\\w?$', '', i) for i in filtered_list]\n",
    "    filtered_sent =' '.join(list_filtered)\n",
    "    clean_sent=re.sub(r'\\s\\s+', ' ', filtered_sent)\n",
    "    #Remove any whitespace at the front of the sentence\n",
    "    clean_sent=clean_sent.lstrip(' ')\n",
    "    return clean_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_comments(filename=''):\n",
    "    comments = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "\n",
    "    with open(filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        header = True\n",
    "        for row in csvReader:\n",
    "            if header:\n",
    "                header = False\n",
    "                continue\n",
    "            ids.append(row[0])\n",
    "            comments.append(data_clean(row[1]))\n",
    "            if len(row) > 2:\n",
    "                classes = []\n",
    "                for i in range(2, 8):\n",
    "                    classes.append(row[i])\n",
    "                labels.append(classes)\n",
    "\n",
    "    ids = np.asarray(ids)\n",
    "    X = np.asarray(comments)\n",
    "    Y = np.asarray(labels, dtype=int)\n",
    "\n",
    "    return (ids, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train, X_train, Y_train = read_comments('./data/train.csv')\n",
    "id_test, X_test, _ = read_comments('./data/test.csv')\n",
    "labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "\n",
    "max_comment = 0\n",
    "for i in range(X_train.shape[0]):                               \n",
    "    l = len(X_train[i].split())\n",
    "    if max_comment < l:\n",
    "        max_comment = l\n",
    "for i in range(X_test.shape[0]):                               \n",
    "    l = len(X_test[i].split())\n",
    "    if max_comment < l:\n",
    "        max_comment = l\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5,random_state=seed)\n",
    "# folds = list()\n",
    "# for train_index, dev_index in kfold.split(X_train):\n",
    "#     folds.append({\n",
    "#         'train_index' : train_index,\n",
    "#         'dev_index' : dev_index,     \n",
    "#     })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec = read_embeddings('./glove/glove.twitter.27B.25d.txt')\n",
    "word_to_vec[\"0.065581\"] = [ 0.39605,  -0.96669,   0.23706,  -0.41379,  -0.97006,   0.16601,  -1.292,\n",
    " -0.58989,   0.11632,  -1.365,    -0.27939,  -0.57222,  -0.97108,  -0.56319,\n",
    " -0.015263, -0.70465,  -0.13867 ,  1.0702 ,  -0.25557  , 0.25122,  -0.87755,\n",
    "  0.70999 ,  0.9118 ,  -0.30077, 0 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in X_train:\n",
    "    words = r.split()\n",
    "    for w in words:\n",
    "        if w not in word_to_index:\n",
    "            if 'fuck' in w:\n",
    "                print(w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from keras.backend import int_shape\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "_EPSILON = K.epsilon()\n",
    "    \n",
    "def custom_loss(y_true, y_pred):\n",
    "    losses = []\n",
    "    y_pred = K.clip(y_pred, _EPSILON, 1.0-_EPSILON)\n",
    "    num_classes = int_shape(y_pred)[1]\n",
    "    for i in range(num_classes):\n",
    "        losses.append(K.mean(-(\n",
    "            y_true[:, i]*K.log(y_pred[:, i]) - (1-y_true[:, i])*K.log(1-y_pred[:, i])\n",
    "        ), axis = -1))\n",
    "    loss = tf.stack(losses)\n",
    "    loss = K.mean(loss, axis=-1)\n",
    "        \n",
    "    return loss\n",
    "    #return tf.convert_to_tensor(losses, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "estimator_baseline = KerasRegressor(build_fn=baseline_model, nb_epoch=1, batch_size=5, verbose=1)\n",
    "results = cross_val_score(estimator_baseline, averages, Y_train, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    X_indices = np.zeros(shape=(m, max_len))\n",
    "    for i in range(m):                               \n",
    "        sentence_words = X[i].lower().split()\n",
    "        if len(sentence_words) > max_len:\n",
    "            print (\"Too many words:\", len(sentence_words))\n",
    "            continue\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            else :\n",
    "                X_indices[i, j] = word_to_index['word']\n",
    "            j = j + 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, max_comment)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe \n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_len = len(word_to_index) + 1                  \n",
    "    emb_dim = word_to_vec[\"cucumber\"].shape[0]  \n",
    "    emb_matrix = np.zeros(shape=(vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(input_shape, num_classes, word_to_vec, word_to_index):\n",
    "    \"\"\"\n",
    "    Create the model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec -- dictionary mapping every word in a vocabulary into its n-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary \n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    sentence_indices = Input(shape=input_shape, dtype='float32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    X = Bidirectional(LSTM(18, return_sequences = False))(embeddings)\n",
    "    X = Dropout(rate=0.1)(X)\n",
    "#     X = LSTM(8, return_sequences = False)(X)\n",
    "#     X = Dropout(rate=0.2)(X)\n",
    "    X = Dense(units=50, activation='relu')(X)\n",
    "    X = Dropout(0.1)(X)\n",
    "    X = Dense(units=num_classes, activation='sigmoid')(X)\n",
    "    model = Model(inputs = [sentence_indices], outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RNN = rnn_model((max_comment,), num_classes, word_to_vec, word_to_index)\n",
    "model_RNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RNN.fit(X_train_indices, Y_train, epochs = 2, batch_size = 128, shuffle=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(probabilities, title):\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    fig.suptitle(title)\n",
    "    plt.autoscale(enable=False, axis='y')\n",
    "    width = 0.85\n",
    "    ind = np.arange(len(probabilities))\n",
    "    plt.bar(ind, probabilities, width=width, color='g')\n",
    "    plt.xticks(ind, labels)\n",
    "    \n",
    "def predict(text, model):\n",
    "    plot_predictions(model.predict(sentences_to_indices(\n",
    "        np.asarray([text]),word_to_index, max_comment))[0], text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Do you think you are smart? idiot\", model_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"I love you, my darling\", model_RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model_RNN.predict(X_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([\n",
    "    pd.DataFrame(id_test, columns=['id']), \n",
    "    pd.DataFrame(y, columns=labels)], axis=1)\n",
    "result = result.set_index('id')\n",
    "result = result.fillna(0.0)\n",
    "result.to_csv('./result/anton_rnn_{}.csv'.format(datetime.now()), header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
